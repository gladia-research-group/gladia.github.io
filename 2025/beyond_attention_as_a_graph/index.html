<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Attention as a Graph | GLADIA </title> <meta name="author" content="GLADIA "> <meta name="description" content="Higher-order (n-simplicial) attention as topology-driven message passing beyond graphs."> <meta name="keywords" content="machine learning, model merging, GLADIA, research"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/blog/assets/img/icon.png?8e1d4d7a89f04601cfbcc2480b271347"> <link rel="stylesheet" href="/blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://crisostomi.github.io/blog/2025/beyond_attention_as_a_graph/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/blog/assets/js/distillpub/template.v2.js"></script> <script src="/blog/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Beyond Attention as a Graph",
            "description": "Higher-order (n-simplicial) attention as topology-driven message passing beyond graphs.",
            "published": "October 09, 2025",
            "authors": [
              
              {
                "author": "Francesco Pappone",
                "authorURL": "https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective",
                "affiliations": [
                  {
                    "name": "Università La Sapienza di Roma -- PSTP Technoscience",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://gladia.di.uniroma1.it/" rel="external nofollow noopener" target="_blank"> GLADIA </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Beyond Attention as a Graph</h1> <p>Higher-order (n-simplicial) attention as topology-driven message passing beyond graphs.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#motivation">Motivation</a> </div> <div> <a href="#depth-for-transformers">Depth for Transformers</a> </div> <div> <a href="#what-lies-beyond-graphs">What lies beyond graphs</a> </div> <div> <a href="#2-simplicial-attention">2-Simplicial Attention</a> </div> <div> <a href="#rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</a> </div> <div> <a href="#so-what-does-n-simplicial-attention-mean-for-depth">So what does n-simplicial attention mean for depth?</a> </div> <div> <a href="#wrapping-up">Wrapping up</a> </div> <div> <a href="#acknowledgements">Acknowledgements</a> </div> <div> <a href="#suggested-citation">Suggested citation</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/beyond-480.webp 480w,/blog/assets/img/blog/beyond_attention/beyond-800.webp 800w,/blog/assets/img/blog/beyond_attention/beyond-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/beyond.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Most attention variants have been designed to retain as much sample efficiency as possible, under the constraint of achieving subquadratic scaling with respect to sequence length.</p> <p>While this has clearly been a powerful research direction, recent changes in the pretraining paradigm have directed <em>attention</em> to architectures capable of <a href="https://www.youtube.com/watch?v=6nJZopACRuQ" rel="external nofollow noopener" target="_blank">increasing sample-efficiency</a>.</p> <p>In my previous <a href="https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective" rel="external nofollow noopener" target="_blank">blogpost</a> I had briefly introduced, as a tool to explain attention sinks, a simple way of viewing attention as a graph operation.</p> <p>We will use this same viewpoint to argue that regular transformers may be <strong>fundamentally limited</strong> in their message-passing capabilities, arguing in favor of higher-order attention methods, such as <a href="https://arxiv.org/abs/2507.02754" rel="external nofollow noopener" target="_blank">2-simplicial Attention</a> and provide a natural way of generalizing it to $n$-simplices, while explaining them from a <strong>topological perspective</strong>.</p> <p>Finally, we will also poke at the very mechanism that makes Machine Learning “deep”: <strong>layer composition</strong>.</p> <h2 id="motivation">Motivation</h2> <p>“Deep Learning” is named after the typical definition of Neural Network models as a set of subsequent, composed <em>Layers</em>.</p> <p>Layers represent atomic, parametric transformations between vector spaces, rendered non-linear by a selection of activation functions.</p> <p>In transformers, layers are organized in transformer blocks, and the two are often used interchangeably. Transformer blocks are nothing more than subsequent attention and MLP transformations operating on the residual stream.</p> <p>Intuitively, depth is easy to justify: while the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="external nofollow noopener" target="_blank">Universal Approximation Theorem</a> guarantees that a single, infinitely wide non-linear layer can approximate any continuous function arbitrarily well, it doesn’t mean that width scaling is practical.</p> <p>As it turns out, properly approximating functions becomes exponentially hard with respect to the dimension of the spaces the functions map between, which can be seen as another angle of the <a href="https://arxiv.org/pdf/2104.13478" rel="external nofollow noopener" target="_blank">curse of dimensionality</a>.</p> <p>For this reason, it becomes convenient to instead “break down” the approximation problem by composing several parametric layers, one after the other.</p> <p>This allows the model to increase in expressivity without exploding in (latent) dimensionality.</p> <p>As for all worthwhile architectural choices in deep learning, this exposes us to a tradeoff: composing operations sequentially is <em>by definition</em> the least <strong>parallel (and hence fast) architectural choice we can make</strong>.</p> <h3 id="depth-for-transformers">Depth for Transformers</h3> <p>While the previous considerations apply in general for all Neural Network architectures, transformers in particular have their specific drawbacks when scaling depth: Transformers’ success has been greatly propelled by their natural parallelism during Next Token Prediction tasks, and, apart from inevitably increasing latency in both inference and training, depth exposes the network to further instability in gradients, as, depending on normalization, the model risks vanishing or exploding gradients.</p> <p>In sequence modelling, though, one key element justifies depth: attention is an operation that message-passes between pairs of tokens in a graph. This means that individual transformer blocks can only possibly encode interactions between pairs of tokens. <strong>Depth allows information to be passed beyond a single-hop</strong>: if we reframe the $AV$ multiplication as in the attention sinks blogpost (seeing as “diffusion” of $V$ on the graph), we can reconnect this intuition to regular graph theory by noticing how powers of the adjacency matrix of a graph, $A^k$, represent $k$-hop walks from each node, and therefore depth approximates this due to attention’s fully connected, yet sparse, input-dependent adjacency matrix.</p> <p>As a result, depth is a fundamental ingredient in transformers that allows them to effectively message-pass between <em>tuples</em> of tokens, and hence build complex and useful representations of tokens in sequences.</p> <p>But what if there existed a way to message-pass between tuples of tokens without resorting to depth?</p> <h2 id="what-lies-beyond-graphs">What lies beyond graphs</h2> <p><a href="https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective" rel="external nofollow noopener" target="_blank">As we know</a>, the message-passing operation happening during attention can be conceptualized as a graph operation. This simple observation, while trivial, has a relevant practical consequence: an entire field of science has, since roughly 2017, been extensively studying Neural Networks as message-passing on graphs, and has developed a variety of theories and techniques to best represent information on topological objects. Of course, the field in question is Geometric Deep Learning, and its central contributions, Graph Neural Networks and Topological Deep Learning.</p> <p>Notably, one key element of that vast literature has been an expressivity bound on GNN architectures: if we define “expressivity” as the capability of distinguishing graphs that are different, then a GNN is only as expressive as the <a href="https://arxiv.org/abs/2201.07083" rel="external nofollow noopener" target="_blank">Weisfeiler-Lehman test</a> (also referred to as the WL-test) . I won’t go in the details of what the test is, and will gladly refer the interested reader to <a href="https://x.com/fedzbar" rel="external nofollow noopener" target="_blank">Federico Barbero’s</a> excellent <a href="https://www.youtube.com/watch?v=AJG1K0dbpes" rel="external nofollow noopener" target="_blank">video</a> explaining it.</p> <p>If you don’t have the time, here’s the gist of it: the WL-test is designed to understand when two graphs are isomorphic (the same graph), but it doesn’t always work. It can be shown that a GNN is <a href="https://arxiv.org/pdf/1810.00826" rel="external nofollow noopener" target="_blank">at most as expressive at graph ismorphism as the WL-test itself</a>.</p> <p>If you’re anything like me, this sounds like bad news: what do you mean we have a theoretically bounded expressivity? Isn’t universal approximation the reason we like Neural Networks so much?</p> <p>Fortunately, not everything is lost. As it turns out, it’s possible to “break” the WL-test bound by inserting higher-order topological information.</p> <p>But what does it mean?</p> <p>As you know, a graph is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ , where $\mathcal{V} = {1, 2, \cdots , n}$ is a set of <em>nodes</em>, and $\mathcal{E}: \mathcal{V} \times \mathcal{V} \rightarrow {0,1}$ is a set of <em>links</em>, also called <em>edges</em> if $(i,j) \in \mathcal{E}$ also implies $(j,i) \in \mathcal{E}$.</p> <p>In other words, elements in $\mathcal{E}$ represent directed, pairwise relations between nodes in the graph.</p> <p>This can be naturally extended by considering a generalization of $\mathcal{E}$, say $\mathcal{E}^{(k)}$, with $k \in \mathbb{N}$, where \(\mathcal{E}^{(k)} : \mathcal{V}^{k+1} \rightarrow \{0,1\}.\)Intuitively, this represents <em>$k$-sized</em> <em>tuples</em> of nodes. For example, for $k=2$, this is equivalent to all <strong>directed triangles</strong> between nodes, while the case $k=1$ recovers the original graph with pairwise links. Note, how, intuitively, for $\mathcal{E}^{(k)}$, we would be effectively considering $k$-dimensional$\,$geometric objects: nodes would be 0-dimensional points, edges 1-dimensional lines, triangles 2-dimensional surfaces, and so on (of course this is just an intuition, for this to be true we would need to embed our nodes in a space and require relations to be undirected).</p> <p>Inserting higher-order information in message passing in GNNs can be shown to increase expressivity beyond the regular WL-test. More generally, <a href="https://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf" rel="external nofollow noopener" target="_blank">it can be shown</a> that the networks with <strong>order</strong> <strong>$k$ topological information are bounded by the $k$-WL test</strong>.</p> <p>While this is by no means a formal introduction to higher-order topological objects like <a href="https://en.wikipedia.org/wiki/Simplicial_complex" rel="external nofollow noopener" target="_blank">Simplicial Complexes</a>, it should be sufficient to paint an intuition about where we’re going: if we manage to message-pass also considering higher order topological objects, instead of just pairs of tokens, we may be able to capture more complex patterns in parallel, instead of having to rely on depth.</p> <h1 id="2-simplicial-attention">2-Simplicial Attention</h1> <p>The Higher-order Attention idea has been floating around for a while: its first implementation in a transformer architecture is dated to <a href="https://arxiv.org/abs/1909.00668" rel="external nofollow noopener" target="_blank">the 2019 work by Clift et al.</a>, and further along has been reinvented/reinterpreted/tangentially rediscovered in a series of works, such as <a href="https://arxiv.org/abs/2306.02896" rel="external nofollow noopener" target="_blank">Representational Strengths and Limitations of Transformers</a>, <a href="https://arxiv.org/abs/2405.16411" rel="external nofollow noopener" target="_blank">Tensor attention</a> , <a href="https://arxiv.org/pdf/2405.14094" rel="external nofollow noopener" target="_blank">The Cellular Transformer</a>, <a href="https://www.nature.com/articles/s41586-021-03819-2" rel="external nofollow noopener" target="_blank">AlphaFold 2</a>, <a href="https://arxiv.org/html/2406.09308v1" rel="external nofollow noopener" target="_blank">TransNAR</a> and i’m sure a bunch of others. Even I, since last year, have been obsessed with the idea, proposing it in public a <a href="https://x.com/tensorqt/status/1841400707515068662" rel="external nofollow noopener" target="_blank">couple</a> of <a href="https://x.com/tensorqt/status/1869997010821992788" rel="external nofollow noopener" target="_blank">times</a>.</p> <p>Apart from theoretical work, what this idea really needed was a step towards experimental validation under a modern paradigm. Fortunately, <a href="https://x.com/aurko79" rel="external nofollow noopener" target="_blank">Aurko</a>, <a href="https://x.com/_arohan_" rel="external nofollow noopener" target="_blank">Rohan</a> and their colleagues delivered well beyond that: a <a href="https://arxiv.org/abs/2507.02754" rel="external nofollow noopener" target="_blank">novel implementation</a> of an Higher-order Attention method was the first architectural change to seem to induce a change in the exponent in the scaling law of Large Language Model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 1: <em>scaling law results from the <a href="https://arxiv.org/abs/2507.02754" rel="external nofollow noopener" target="_blank">Fast and Simplex: 2-Simplicial Attention in Triton</a> paper.</em></p> <h2 id="rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</h2> <p>So, how do we extend our graph-based perspective on attention, so that it naturally becomes a (potentially higher-order) topological perspective?</p> <p>Refreshing the graph case, let’s take, for example \(X \in \mathbb{R}^{\,n\times d}\)</p> <p>And let’s treat the rows of $X$ as a <strong>point‑cloud</strong>: \(X = \begin{bmatrix} x_1^{\!\top}\\ x_2^{\!\top}\\ \vdots\\ x_n^{\!\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\)</p> <p>Constructing the $Q$,$K$,$V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{\,n\times d_q}\] \[K = X W_k \in \mathbb{R}^{\,n\times d_q}\] \[V = X W_v \in \mathbb{R}^{\,n\times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong> \(\alpha_{ij} \;=\; \langle q_i, k_j\rangle \;=\; q_i k_j^{\!\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\) Stacking all scores: \(\alpha \;=\; Q K^{\!\top} \in \mathbb{R}^{\,n\times n}.\)</p> <p>The intuition is: the more points align in Query - Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig 2: <em>an attention matrix encodes a graph</em></p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp\!\bigl(\alpha_{ij}/\sqrt{d_k}\bigr)} {\displaystyle\sum_{j'=1}^n \exp\!\bigl(\alpha_{ij'}/\sqrt{d_k}\bigr)}\,, \qquad A = \mathrm{softmax}\!\Bigl(\tfrac{\alpha}{\sqrt{d_k}}\Bigr)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\mathrm{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graphs</strong> which is diffused from its neighbors to each node.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 3: <em>left: central node ($i$) weighs via attention neighboring nodes; right: central node aggregates via attention weights the value function defined on neighboring nodes</em></p> <p>But we already knew all of this from the previous blogpost. The key point to notice, here, is the operation we perform to extract a graph: we project $X$ into two distinct spaces via $W_Q$ and $W_K$, precisely because we need to perform a <em>bi</em>-linear form (the dot product) to extract a two-way relationship.</p> <p>What if we wanted to capture three-way relationships? Naturally, one could think of adding a second $K^{\prime}$ matrix, resulting from a $W_{K^{\prime}}$ projection, such that we would have a 3D tensor \(T_{ijk} = \sum_{l}Q_{il}K_{jl}K^{\prime}_{kl}\) Which can also be seen as taking a multilinear product, if viewed per query: \(T_{ijs} = \langle q_i, k_j, k^{\prime}_s \rangle\)</p> <p>Notice how, before, each attention score $A_{ij}$ represented the link weight going from node $i$ to node $j$. Now, each entry $T_{ijk}$ can instead be seen as the collective weight assigned to the triangle determined by the (directed) walk from node $i$, passing through node $j$, and ending up in node $k$.<br> Such a triangle, in algebraic topology, may also be called a <em>2-simplex</em> (a node is a 0-simplex, an edge is a 1-simplex), explaining the naming of the attention mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 4: <em>2-simplicial attention’s tensor T, in each of its entries, represents a (directed) 2-simplex (triangle)</em></p> <p>Now that we’ve found a formulation to represents 2-simplices (or simplexes, one day i’ll have to decide which version of the plural i prefer), how do we transfer our regular sparsification mechanism (softmax) to it? And, moreover, what even is a neighborhood in this case?</p> <p>The intuitive extension of attentions (also used in 2-simplicial attention) treats this by keeping the query token as central: instead of being a matrix, our attention score is now a 3D tensor. This simply means that, instead of rows, we now normalize over entire slices associated with query $i$.</p> <p>Meaning, our softmax operation becomes: \(\alpha^{(2)}_{ijk}= \mathrm{softmax(T)}^{(2)}_{ijk} = \frac{e^{T_{ijk}}}{\sum_{jk}e^{T_{ijk}}}\) Intuitively, this is defining the node’s neighborhood as the <strong>triangles it’s included in</strong>. Hence, here, we’re squashing to zero triangles with low three-way similarity, and amplifying the signal from the more similar ones.</p> <p>This makes sense because our final goal will be to use this information to update the nodes’ embeddings. With that said, there exist more ways to define adjacency for higher order structure: an interesting idea could be to normalize over triangles sharing faces, instead.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 5: <em>left: message passing now happens between 2-simplices (oriented triangles). Each 2-simplex is weighed by an entry in tensor T. Right: each 2-simplex has an aggregated value vector that is used to update the node’s representation</em></p> <p>The last piece of the puzzle is the $V$ matrix of regular attention. As we discussed previously, it can be thought of as a vector-valued function defined on nodes, where individual vectors are rows $V_i$.</p> <p>So what about 2-simplicial attention? Naturally, $V$ would still have to be defined token-wise, but now we have to engineer it so that it can represent, for node $i$, the value associated with the neighbors in a triangle, just like in regular attention $V$ was being aggregated from neighbors in the graph. Furthermore, in order to express value of tokens with full degrees of freedom, we introduce a second value projection, $V^{\prime}$, that we use analogously to $K^{\prime}$. What we need is for all triangles $(i,j,k)$ to aggregate $V_j$ and $V_{k}^{\prime}$ with some function $f:\mathbb{R}^{h}\times \mathbb{R}^{h} \rightarrow \mathbb{R}^{h}$. such that we have, for each triangle, a resulting vector \(V^{(2)}_{ijk} = f(V_{k},V_{k}^{\prime})\). In the paper, f is just the product of the entries of $V$, which can be conveniently written as an element-wise product between $V$ and $V^{\prime}$: \(V^{(2)}_{ijk} = V_{ik}V_{jk}^{\prime}\) Apart from convenience, this choice can also be seen as combining value vectors using an “AND” operation, in the sense that large values will compound, and a single small value is sufficient to drop the magnitude of the vector. This is opposed, for example, to having the function be \(V^{(2)}_{ijk} = V_{ik}+ V_{jk}^{\prime}\) which would, instead, be analogous to an “OR” operation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 6: <em>$v$ and $v’$ from each triangle are aggregated and used to update the central node’s embedding</em></p> <p>At last, we end up with $V^{(2)}$ being another 3D tensor. This allows us to perform the final operation of attention as a tensor contraction taking us back to our regular $\mathbb{R}^{n\times d}$ shape: \(\mathrm{attention(x)}_{il} = \sum_{jk}\frac{\alpha^{(2)}_{ijk}V^{(2)}_{jkl}}{\sqrt{d}}\) Note how this operation can still be thought of as some kind of “diffusion”: we are aggregating value vectors from each triangle including node $i$, scaling them and summing them to update the vector in node $i$.</p> <p>Now, the extension to the n-simplicial case is trivial: For n-simplices, we just repeat the 2-simplicial recipe with $n$ Key projections. For an $(n+1)$-tuple $(i,j_1,\ldots,j_n)$ define the score tensor by a multilinear form</p> \[T_{i\,j_1\cdots j_n} \;=\; \sum_{\ell} Q_{i\ell}\;\prod_{m=1}^n K^{(m)}_{j_m\ell} \;=\;\langle q_i, k^{(1)}_{j_1},\ldots,k^{(n)}_{j_n}\rangle,\] <p>and normalize per-query over all $n$-tuples to get</p> \[\alpha^{(n)}_{i\,j_1\cdots j_n} \;=\; \frac{\exp T_{i\,j_1\cdots j_n}}{\sum_{(j_1,\ldots,j_n)} \exp T_{i\,j_1\cdots j_n}}.\] <p>Values remain token-wise but are combined along each $n$-simplex via a symmetric $n$-ary reducer $f$ ; the simplest is the element-wise product “AND”</p> \[V^{(n)}_{i\,j_1\cdots j_n} \;=\; \prod_{m=1}^{n} V^{[m]}_{j_m i},\] <p>though sum/mean (an “OR”) or MLP reducers are possible. The update is then a contraction over all $n$-tuples incident to $i$ :</p> \[\mathrm{attn}(X)_{i\ell}\;=\;\frac{1}{\sqrt{d}}\sum_{j_1,\ldots,j_n}\alpha^{(n)}_{i\,j_1\cdots j_n}\; \big[V^{(n)}_{j_1\cdots j_n}\big]_\ell\] <p>Topologically, we’re diffusing over the star of $i$ in the $n$-skeleton (cofaces incident to $i$ ), so higher-order interactions are captured in one hop.</p> <p>Naturally, an $n$-simplicial attention mechanism’s memory scales catastrophically quickly with sequence length, precisely with $O(L^{n+1})$. This means that we have to come up with ways of saprsifying this mechanism in order to make it practical.</p> <p>In the 2-simplicial attention paper, this is solved by performing Sliding Window Attention (SWA) with potentially different windows per dimension in the attention tensor.</p> <p>But is this the only way to tackle this? When i first started pondering these ideas, my first thought was instead to route tokens dynamically to a fixed size window. A very similar idea came recently with <a href="https://api-docs.deepseek.com/news/news250929" rel="external nofollow noopener" target="_blank">Deepseek 3.2</a>, in the shape of DeepSeek Sparse Attention (DSA). The intuition is simple: why have a sliding window when you can hand-pick the tokens you want to use, with your preferred sparsity?</p> <p>DSA (DeepSeek Sparse Attention) replaces dense attention with a two-stage sparse mechanism: a <strong>lightweight indexer</strong> followed by <strong>top-k token selection</strong>.</p> <p>The indexer computes cheap similarity scores between each query and all past tokens. For each query token $i$ and indexer head $h$, it first computes</p> \[s_{i j h} \;=\; \mathrm{ReLU}(\langle q^I_{i h},\, k^I_{j} \rangle) \cdot w^I_{i h},\] <p>where $q^I_{i h}$ is the indexer’s query vector for token $i$ and head $h$, $k^I_j$ is the (shared) indexer key for token $j$, and $w^I_{i h}$ is a learned per-head weight. Summing over heads gives the final score</p> \[S_{ij} \;=\; \sum_{h=1}^{H_I} s_{i j h}.\] <p>For each query $i$, the top-k keys according to $S_{ij}$ are selected:</p> \[\mathcal{K}_i \;=\; \mathrm{TopK}_j \big(S_{ij},\, k\big),\] <p>and full attention is then computed <strong>only</strong> on this restricted set.</p> <p>This reduces the core attention complexity from $O(L^2)$ to $O(Lk)$, while preserving the most relevant interactions, making it particularly effective for long contexts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 7: <em>intuitively representing full attention, SWA and DSA in the regular case</em></p> <p>In our case, we use a modified version of DSA to substitute SWA: first, we notice that substituting ReLU with softmax performs better on our small experiments on a token-wise level. Furthermore, to avoid individual computation of $qk_1^T$ and $qk_2^T$ distinct pairs, we instead leverage existing $QK^T$ from the previous regular attention layers, and directly index based on those scores, obtaining the same exact top-k scorers for both $k_1$ and $k_2$.</p> <p>This yields a tiny speedup to our very small model / small token-horizon run, while keeping the same scaling as SWA, where we have $O(Lk^2)$ (with $k$ chosen to be equivalent to the window size of SWA) instead of the full sequence $O(L^3)$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 8: <em>losses for a 127M variant of nanogpt using a 3:1 regular to 2-simplicial attention ratio, with a block size of 512 and a top-k/SWA window of 128 tokens. In gray, is the SWA-sparsified version, in green the DSA-inspired technique we introduced. In orange, regular self-attention.</em> Total token horizon is of around 60M tokens.</p> <p>While in Fig. 8 we can see that baseline appears to have roughly the same acceleration as windowed simplicial attention, we notice how the 2-simplicial attention paper itself only notices gains against the transformer at a much larger parameter size, as seen in Fig. 10.</p> <p>Overall, though, our acceleration is of an average of <strong>0.76%</strong> (so barely noticeable) with respect to the Sliding Window Attention version. In Fig.9, we can see (batch-wise) speedup in training:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/output%20(41)-480.webp 480w,/blog/assets/img/blog/beyond_attention/output%20(41)-800.webp 800w,/blog/assets/img/blog/beyond_attention/output%20(41)-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/output%20(41).png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig. 9: <em>speedup across steps of DSA and SWA vs baseline</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Fig.10: <em>reported performance comparison between transformers and 2-simplicial attention in the original paper.</em></p> <h3 id="so-what-does-n-simplicial-attention-mean-for-depth">So what does $n$-simplicial attention mean for depth?</h3> <p>As we’ve discussed, one of the key elements of depth is <strong>multi-token representation-learning</strong>. Another way to view it, is that individual tokens are in a <strong>constant relay race</strong>: each token wants to get to a target representation, but needs crucial information from other tokens’ representations to do so. If the proper representation is very hard to find, the model eventually runs out of depth to message-pass. 2-simplicial attention goes in the direction of fixing this, because it <strong>combinatorially opens up surface area</strong> for the model to do message-passing, for each block. Of course, the present one is just its first, prototypal iteration, which will inevitably change in the future (us at <a href="https://gladia.netlify.app/" rel="external nofollow noopener" target="_blank">Gladia</a> are already hard at work).</p> <h3 id="wrapping-up">Wrapping up</h3> <p>We’ve explored a recent advance in attention architecture, and explained it using our previously established topologically-oriented angle. We’ve also outlined a trivial extension to n-simplices of the mechanism, as well as demonstrated tiny gains in expressivity by utilizing a DSA-like sparsification of 2-simplicial attention keys, substituting SWA. Given my obsession with the topic, you’re very likely to read something from me on the topic soon. In the meantime, let me know what you think!</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>Thanks a lot to <a href="https://x.com/thelokasiffers" rel="external nofollow noopener" target="_blank">thelakosiffers</a>, <a href="https://x.com/leothecurious" rel="external nofollow noopener" target="_blank">davinci</a> <a href="https://x.com/MatteoManias" rel="external nofollow noopener" target="_blank">MatteoManias</a>, <a href="https://x.com/f14bertolotti" rel="external nofollow noopener" target="_blank">Francesco Bertolotti</a>, <a href="https://x.com/_ueaj" rel="external nofollow noopener" target="_blank">ueaj</a>, <a href="https://x.com/biiordache" rel="external nofollow noopener" target="_blank">Bianca</a>, <a href="https://x.com/aurko79" rel="external nofollow noopener" target="_blank">Aurko</a>,<a href="https://x.com/_arohan_" rel="external nofollow noopener" target="_blank">Rohan</a> , <a href="https://x.com/mike64_t" rel="external nofollow noopener" target="_blank">Mike</a> , <a href="https://x.com/borak_004" rel="external nofollow noopener" target="_blank">Borak</a>, <a href="https://x.com/graffioh" rel="external nofollow noopener" target="_blank">Berto</a> and <a href="https://x.com/Niccolg92" rel="external nofollow noopener" target="_blank">Niccolò Gentile</a> for their precious feedback!</p> <hr> <h3 id="suggested-citation">Suggested citation</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">pappone2025beyondattention</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Francesco Pappone}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Beyond Attention as  Graph}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="s">{October}</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{09}</span><span class="p">,</span>
  <span class="na">institution</span>  <span class="p">=</span> <span class="s">{Università La Sapienza di Roma -- PSTP Technoscience}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://publish.obsidian.md/the-tensor-throne/The+Graph+Side+of+Attention/Beyond+Attention+as+a+Graph}}</span><span class="p">,</span> 
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{Blogpost}</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/blog/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/model_merging/">Model Merging — a biased overview</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/attention_sinks/">Attention sinks from the graph perspective</a> </li> <br> <br> <div id="giscus_thread"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'gladia-research-group/blog',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 GLADIA . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/blog/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/blog/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/blog/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>