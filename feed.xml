<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://crisostomi.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://crisostomi.github.io/blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-22T10:55:02+00:00</updated><id>https://crisostomi.github.io/blog/feed.xml</id><title type="html">GLADIA</title><subtitle></subtitle><entry><title type="html">Beyond Attention as a Graph</title><link href="https://crisostomi.github.io/blog/2025/beyond_attention_as_a_graph/" rel="alternate" type="text/html" title="Beyond Attention as a Graph"/><published>2025-10-09T00:00:00+00:00</published><updated>2025-10-09T00:00:00+00:00</updated><id>https://crisostomi.github.io/blog/2025/beyond_attention_as_a_graph</id><content type="html" xml:base="https://crisostomi.github.io/blog/2025/beyond_attention_as_a_graph/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/beyond-480.webp 480w,/blog/assets/img/blog/beyond_attention/beyond-800.webp 800w,/blog/assets/img/blog/beyond_attention/beyond-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/beyond.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Most attention variants have been designed to retain as much sample efficiency as possible, under the constraint of achieving subquadratic scaling with respect to sequence length.</p> <p>While this has clearly been a powerful research direction, recent changes in the pretraining paradigm have directed <em>attention</em> to architectures capable of <a href="https://www.youtube.com/watch?v=6nJZopACRuQ">increasing sample-efficiency</a>.</p> <p>In my previous <a href="https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective">blogpost</a> I had briefly introduced, as a tool to explain attention sinks, a simple way of viewing attention as a graph operation.</p> <p>We will use this same viewpoint to argue that regular transformers may be <strong>fundamentally limited</strong> in their message-passing capabilities, arguing in favor of higher-order attention methods, such as <a href="https://arxiv.org/abs/2507.02754">2-simplicial Attention</a> and provide a natural way of generalizing it to $n$-simplices, while explaining them from a <strong>topological perspective</strong>.</p> <p>Finally, we will also poke at the very mechanism that makes Machine Learning “deep”: <strong>layer composition</strong>.</p> <h2 id="motivation">Motivation</h2> <p>“Deep Learning” is named after the typical definition of Neural Network models as a set of subsequent, composed <em>Layers</em>.</p> <p>Layers represent atomic, parametric transformations between vector spaces, rendered non-linear by a selection of activation functions.</p> <p>In transformers, layers are organized in transformer blocks, and the two are often used interchangeably. Transformer blocks are nothing more than subsequent attention and MLP transformations operating on the residual stream.</p> <p>Intuitively, depth is easy to justify: while the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a> guarantees that a single, infinitely wide non-linear layer can approximate any continuous function arbitrarily well, it doesn’t mean that width scaling is practical.</p> <p>As it turns out, properly approximating functions becomes exponentially hard with respect to the dimension of the spaces the functions map between, which can be seen as another angle of the <a href="https://arxiv.org/pdf/2104.13478">curse of dimensionality</a>.</p> <p>For this reason, it becomes convenient to instead “break down” the approximation problem by composing several parametric layers, one after the other.</p> <p>This allows the model to increase in expressivity without exploding in (latent) dimensionality.</p> <p>As for all worthwhile architectural choices in deep learning, this exposes us to a tradeoff: composing operations sequentially is <em>by definition</em> the least <strong>parallel (and hence fast) architectural choice we can make</strong>.</p> <h3 id="depth-for-transformers">Depth for Transformers</h3> <p>While the previous considerations apply in general for all Neural Network architectures, transformers in particular have their specific drawbacks when scaling depth: Transformers’ success has been greatly propelled by their natural parallelism during Next Token Prediction tasks, and, apart from inevitably increasing latency in both inference and training, depth exposes the network to further instability in gradients, as, depending on normalization, the model risks vanishing or exploding gradients.</p> <p>In sequence modelling, though, one key element justifies depth: attention is an operation that message-passes between pairs of tokens in a graph. This means that individual transformer blocks can only possibly encode interactions between pairs of tokens. <strong>Depth allows information to be passed beyond a single-hop</strong>: if we reframe the $AV$ multiplication as in the attention sinks blogpost (seeing as “diffusion” of $V$ on the graph), we can reconnect this intuition to regular graph theory by noticing how powers of the adjacency matrix of a graph, $A^k$, represent $k$-hop walks from each node, and therefore depth approximates this due to attention’s fully connected, yet sparse, input-dependent adjacency matrix.</p> <p>As a result, depth is a fundamental ingredient in transformers that allows them to effectively message-pass between <em>tuples</em> of tokens, and hence build complex and useful representations of tokens in sequences.</p> <p>But what if there existed a way to message-pass between tuples of tokens without resorting to depth?</p> <h2 id="what-lies-beyond-graphs">What lies beyond graphs</h2> <p><a href="https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective">As we know</a>, the message-passing operation happening during attention can be conceptualized as a graph operation. This simple observation, while trivial, has a relevant practical consequence: an entire field of science has, since roughly 2017, been extensively studying Neural Networks as message-passing on graphs, and has developed a variety of theories and techniques to best represent information on topological objects. Of course, the field in question is Geometric Deep Learning, and its central contributions, Graph Neural Networks and Topological Deep Learning.</p> <p>Notably, one key element of that vast literature has been an expressivity bound on GNN architectures: if we define “expressivity” as the capability of distinguishing graphs that are different, then a GNN is only as expressive as the <a href="https://arxiv.org/abs/2201.07083">Weisfeiler-Lehman test</a> (also referred to as the WL-test) . I won’t go in the details of what the test is, and will gladly refer the interested reader to <a href="https://x.com/fedzbar">Federico Barbero’s</a> excellent <a href="https://www.youtube.com/watch?v=AJG1K0dbpes">video</a> explaining it.</p> <p>If you don’t have the time, here’s the gist of it: the WL-test is designed to understand when two graphs are isomorphic (the same graph), but it doesn’t always work. It can be shown that a GNN is <a href="https://arxiv.org/pdf/1810.00826">at most as expressive at graph ismorphism as the WL-test itself</a>.</p> <p>If you’re anything like me, this sounds like bad news: what do you mean we have a theoretically bounded expressivity? Isn’t universal approximation the reason we like Neural Networks so much?</p> <p>Fortunately, not everything is lost. As it turns out, it’s possible to “break” the WL-test bound by inserting higher-order topological information.</p> <p>But what does it mean?</p> <p>As you know, a graph is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ , where $\mathcal{V} = {1, 2, \cdots , n}$ is a set of <em>nodes</em>, and $\mathcal{E}: \mathcal{V} \times \mathcal{V} \rightarrow {0,1}$ is a set of <em>links</em>, also called <em>edges</em> if $(i,j) \in \mathcal{E}$ also implies $(j,i) \in \mathcal{E}$.</p> <p>In other words, elements in $\mathcal{E}$ represent directed, pairwise relations between nodes in the graph.</p> <p>This can be naturally extended by considering a generalization of $\mathcal{E}$, say $\mathcal{E}^{(k)}$, with $k \in \mathbb{N}$, where \(\mathcal{E}^{(k)} : \mathcal{V}^{k+1} \rightarrow \{0,1\}.\)Intuitively, this represents <em>$k$-sized</em> <em>tuples</em> of nodes. For example, for $k=2$, this is equivalent to all <strong>directed triangles</strong> between nodes, while the case $k=1$ recovers the original graph with pairwise links. Note, how, intuitively, for $\mathcal{E}^{(k)}$, we would be effectively considering $k$-dimensional$\,$geometric objects: nodes would be 0-dimensional points, edges 1-dimensional lines, triangles 2-dimensional surfaces, and so on (of course this is just an intuition, for this to be true we would need to embed our nodes in a space and require relations to be undirected).</p> <p>Inserting higher-order information in message passing in GNNs can be shown to increase expressivity beyond the regular WL-test. More generally, <a href="https://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf">it can be shown</a> that the networks with <strong>order</strong> <strong>$k$ topological information are bounded by the $k$-WL test</strong>.</p> <p>While this is by no means a formal introduction to higher-order topological objects like <a href="https://en.wikipedia.org/wiki/Simplicial_complex">Simplicial Complexes</a>, it should be sufficient to paint an intuition about where we’re going: if we manage to message-pass also considering higher order topological objects, instead of just pairs of tokens, we may be able to capture more complex patterns in parallel, instead of having to rely on depth.</p> <h1 id="2-simplicial-attention">2-Simplicial Attention</h1> <p>The Higher-order Attention idea has been floating around for a while: its first implementation in a transformer architecture is dated to <a href="https://arxiv.org/abs/1909.00668">the 2019 work by Clift et al.</a>, and further along has been reinvented/reinterpreted/tangentially rediscovered in a series of works, such as <a href="https://arxiv.org/abs/2306.02896">Representational Strengths and Limitations of Transformers</a>, <a href="https://arxiv.org/abs/2405.16411">Tensor attention</a> , <a href="https://arxiv.org/pdf/2405.14094">The Cellular Transformer</a>, <a href="https://www.nature.com/articles/s41586-021-03819-2">AlphaFold 2</a>, <a href="https://arxiv.org/html/2406.09308v1">TransNAR</a> and i’m sure a bunch of others. Even I, since last year, have been obsessed with the idea, proposing it in public a <a href="https://x.com/tensorqt/status/1841400707515068662">couple</a> of <a href="https://x.com/tensorqt/status/1869997010821992788">times</a>.</p> <p>Apart from theoretical work, what this idea really needed was a step towards experimental validation under a modern paradigm. Fortunately, <a href="https://x.com/aurko79">Aurko</a>, <a href="https://x.com/_arohan_">Rohan</a> and their colleagues delivered well beyond that: a <a href="https://arxiv.org/abs/2507.02754">novel implementation</a> of an Higher-order Attention method was the first architectural change to seem to induce a change in the exponent in the scaling law of Large Language Model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009140809.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 1: <em>scaling law results from the <a href="https://arxiv.org/abs/2507.02754">Fast and Simplex: 2-Simplicial Attention in Triton</a> paper.</em></p> <h2 id="rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</h2> <p>So, how do we extend our graph-based perspective on attention, so that it naturally becomes a (potentially higher-order) topological perspective?</p> <p>Refreshing the graph case, let’s take, for example \(X \in \mathbb{R}^{\,n\times d}\)</p> <p>And let’s treat the rows of $X$ as a <strong>point‑cloud</strong>: \(X = \begin{bmatrix} x_1^{\!\top}\\ x_2^{\!\top}\\ \vdots\\ x_n^{\!\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\)</p> <p>Constructing the $Q$,$K$,$V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{\,n\times d_q}\] \[K = X W_k \in \mathbb{R}^{\,n\times d_q}\] \[V = X W_v \in \mathbb{R}^{\,n\times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong> \(\alpha_{ij} \;=\; \langle q_i, k_j\rangle \;=\; q_i k_j^{\!\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\) Stacking all scores: \(\alpha \;=\; Q K^{\!\top} \in \mathbb{R}^{\,n\times n}.\)</p> <p>The intuition is: the more points align in Query - Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009231935.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig 2: <em>an attention matrix encodes a graph</em></p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp\!\bigl(\alpha_{ij}/\sqrt{d_k}\bigr)} {\displaystyle\sum_{j'=1}^n \exp\!\bigl(\alpha_{ij'}/\sqrt{d_k}\bigr)}\,, \qquad A = \mathrm{softmax}\!\Bigl(\tfrac{\alpha}{\sqrt{d_k}}\Bigr)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\mathrm{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graphs</strong> which is diffused from its neighbors to each node.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232055.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 3: <em>left: central node ($i$) weighs via attention neighboring nodes; right: central node aggregates via attention weights the value function defined on neighboring nodes</em></p> <p>But we already knew all of this from the previous blogpost. The key point to notice, here, is the operation we perform to extract a graph: we project $X$ into two distinct spaces via $W_Q$ and $W_K$, precisely because we need to perform a <em>bi</em>-linear form (the dot product) to extract a two-way relationship.</p> <p>What if we wanted to capture three-way relationships? Naturally, one could think of adding a second $K^{\prime}$ matrix, resulting from a $W_{K^{\prime}}$ projection, such that we would have a 3D tensor \(T_{ijk} = \sum_{l}Q_{il}K_{jl}K^{\prime}_{kl}\) Which can also be seen as taking a multilinear product, if viewed per query: \(T_{ijs} = \langle q_i, k_j, k^{\prime}_s \rangle\)</p> <p>Notice how, before, each attention score $A_{ij}$ represented the link weight going from node $i$ to node $j$. Now, each entry $T_{ijk}$ can instead be seen as the collective weight assigned to the triangle determined by the (directed) walk from node $i$, passing through node $j$, and ending up in node $k$.<br/> Such a triangle, in algebraic topology, may also be called a <em>2-simplex</em> (a node is a 0-simplex, an edge is a 1-simplex), explaining the naming of the attention mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232001.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 4: <em>2-simplicial attention’s tensor T, in each of its entries, represents a (directed) 2-simplex (triangle)</em></p> <p>Now that we’ve found a formulation to represents 2-simplices (or simplexes, one day i’ll have to decide which version of the plural i prefer), how do we transfer our regular sparsification mechanism (softmax) to it? And, moreover, what even is a neighborhood in this case?</p> <p>The intuitive extension of attentions (also used in 2-simplicial attention) treats this by keeping the query token as central: instead of being a matrix, our attention score is now a 3D tensor. This simply means that, instead of rows, we now normalize over entire slices associated with query $i$.</p> <p>Meaning, our softmax operation becomes: \(\alpha^{(2)}_{ijk}= \mathrm{softmax(T)}^{(2)}_{ijk} = \frac{e^{T_{ijk}}}{\sum_{jk}e^{T_{ijk}}}\) Intuitively, this is defining the node’s neighborhood as the <strong>triangles it’s included in</strong>. Hence, here, we’re squashing to zero triangles with low three-way similarity, and amplifying the signal from the more similar ones.</p> <p>This makes sense because our final goal will be to use this information to update the nodes’ embeddings. With that said, there exist more ways to define adjacency for higher order structure: an interesting idea could be to normalize over triangles sharing faces, instead.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232130.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 5: <em>left: message passing now happens between 2-simplices (oriented triangles). Each 2-simplex is weighed by an entry in tensor T. Right: each 2-simplex has an aggregated value vector that is used to update the node’s representation</em></p> <p>The last piece of the puzzle is the $V$ matrix of regular attention. As we discussed previously, it can be thought of as a vector-valued function defined on nodes, where individual vectors are rows $V_i$.</p> <p>So what about 2-simplicial attention? Naturally, $V$ would still have to be defined token-wise, but now we have to engineer it so that it can represent, for node $i$, the value associated with the neighbors in a triangle, just like in regular attention $V$ was being aggregated from neighbors in the graph. Furthermore, in order to express value of tokens with full degrees of freedom, we introduce a second value projection, $V^{\prime}$, that we use analogously to $K^{\prime}$. What we need is for all triangles $(i,j,k)$ to aggregate $V_j$ and $V_{k}^{\prime}$ with some function $f:\mathbb{R}^{h}\times \mathbb{R}^{h} \rightarrow \mathbb{R}^{h}$. such that we have, for each triangle, a resulting vector \(V^{(2)}_{ijk} = f(V_{k},V_{k}^{\prime})\). In the paper, f is just the product of the entries of $V$, which can be conveniently written as an element-wise product between $V$ and $V^{\prime}$: \(V^{(2)}_{ijk} = V_{ik}V_{jk}^{\prime}\) Apart from convenience, this choice can also be seen as combining value vectors using an “AND” operation, in the sense that large values will compound, and a single small value is sufficient to drop the magnitude of the vector. This is opposed, for example, to having the function be \(V^{(2)}_{ijk} = V_{ik}+ V_{jk}^{\prime}\) which would, instead, be analogous to an “OR” operation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232210.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 6: <em>$v$ and $v’$ from each triangle are aggregated and used to update the central node’s embedding</em></p> <p>At last, we end up with $V^{(2)}$ being another 3D tensor. This allows us to perform the final operation of attention as a tensor contraction taking us back to our regular $\mathbb{R}^{n\times d}$ shape: \(\mathrm{attention(x)}_{il} = \sum_{jk}\frac{\alpha^{(2)}_{ijk}V^{(2)}_{jkl}}{\sqrt{d}}\) Note how this operation can still be thought of as some kind of “diffusion”: we are aggregating value vectors from each triangle including node $i$, scaling them and summing them to update the vector in node $i$.</p> <p>Now, the extension to the n-simplicial case is trivial: For n-simplices, we just repeat the 2-simplicial recipe with $n$ Key projections. For an $(n+1)$-tuple $(i,j_1,\ldots,j_n)$ define the score tensor by a multilinear form</p> \[T_{i\,j_1\cdots j_n} \;=\; \sum_{\ell} Q_{i\ell}\;\prod_{m=1}^n K^{(m)}_{j_m\ell} \;=\;\langle q_i, k^{(1)}_{j_1},\ldots,k^{(n)}_{j_n}\rangle,\] <p>and normalize per-query over all $n$-tuples to get</p> \[\alpha^{(n)}_{i\,j_1\cdots j_n} \;=\; \frac{\exp T_{i\,j_1\cdots j_n}}{\sum_{(j_1,\ldots,j_n)} \exp T_{i\,j_1\cdots j_n}}.\] <p>Values remain token-wise but are combined along each $n$-simplex via a symmetric $n$-ary reducer $f$ ; the simplest is the element-wise product “AND”</p> \[V^{(n)}_{i\,j_1\cdots j_n} \;=\; \prod_{m=1}^{n} V^{[m]}_{j_m i},\] <p>though sum/mean (an “OR”) or MLP reducers are possible. The update is then a contraction over all $n$-tuples incident to $i$ :</p> \[\mathrm{attn}(X)_{i\ell}\;=\;\frac{1}{\sqrt{d}}\sum_{j_1,\ldots,j_n}\alpha^{(n)}_{i\,j_1\cdots j_n}\; \big[V^{(n)}_{j_1\cdots j_n}\big]_\ell\] <p>Topologically, we’re diffusing over the star of $i$ in the $n$-skeleton (cofaces incident to $i$ ), so higher-order interactions are captured in one hop.</p> <p>Naturally, an $n$-simplicial attention mechanism’s memory scales catastrophically quickly with sequence length, precisely with $O(L^{n+1})$. This means that we have to come up with ways of saprsifying this mechanism in order to make it practical.</p> <p>In the 2-simplicial attention paper, this is solved by performing Sliding Window Attention (SWA) with potentially different windows per dimension in the attention tensor.</p> <p>But is this the only way to tackle this? When i first started pondering these ideas, my first thought was instead to route tokens dynamically to a fixed size window. A very similar idea came recently with <a href="https://api-docs.deepseek.com/news/news250929">Deepseek 3.2</a>, in the shape of DeepSeek Sparse Attention (DSA). The intuition is simple: why have a sliding window when you can hand-pick the tokens you want to use, with your preferred sparsity?</p> <p>DSA (DeepSeek Sparse Attention) replaces dense attention with a two-stage sparse mechanism: a <strong>lightweight indexer</strong> followed by <strong>top-k token selection</strong>.</p> <p>The indexer computes cheap similarity scores between each query and all past tokens. For each query token $i$ and indexer head $h$, it first computes</p> \[s_{i j h} \;=\; \mathrm{ReLU}(\langle q^I_{i h},\, k^I_{j} \rangle) \cdot w^I_{i h},\] <p>where $q^I_{i h}$ is the indexer’s query vector for token $i$ and head $h$, $k^I_j$ is the (shared) indexer key for token $j$, and $w^I_{i h}$ is a learned per-head weight. Summing over heads gives the final score</p> \[S_{ij} \;=\; \sum_{h=1}^{H_I} s_{i j h}.\] <p>For each query $i$, the top-k keys according to $S_{ij}$ are selected:</p> \[\mathcal{K}_i \;=\; \mathrm{TopK}_j \big(S_{ij},\, k\big),\] <p>and full attention is then computed <strong>only</strong> on this restricted set.</p> <p>This reduces the core attention complexity from $O(L^2)$ to $O(Lk)$, while preserving the most relevant interactions, making it particularly effective for long contexts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251009232243.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 7: <em>intuitively representing full attention, SWA and DSA in the regular case</em></p> <p>In our case, we use a modified version of DSA to substitute SWA: first, we notice that substituting ReLU with softmax performs better on our small experiments on a token-wise level. Furthermore, to avoid individual computation of $qk_1^T$ and $qk_2^T$ distinct pairs, we instead leverage existing $QK^T$ from the previous regular attention layers, and directly index based on those scores, obtaining the same exact top-k scorers for both $k_1$ and $k_2$.</p> <p>This yields a tiny speedup to our very small model / small token-horizon run, while keeping the same scaling as SWA, where we have $O(Lk^2)$ (with $k$ chosen to be equivalent to the window size of SWA) instead of the full sequence $O(L^3)$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016185211.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 8: <em>losses for a 127M variant of nanogpt using a 3:1 regular to 2-simplicial attention ratio, with a block size of 512 and a top-k/SWA window of 128 tokens. In gray, is the SWA-sparsified version, in green the DSA-inspired technique we introduced. In orange, regular self-attention.</em> Total token horizon is of around 60M tokens.</p> <p>While in Fig. 8 we can see that baseline appears to have roughly the same acceleration as windowed simplicial attention, we notice how the 2-simplicial attention paper itself only notices gains against the transformer at a much larger parameter size, as seen in Fig. 10.</p> <p>Overall, though, our acceleration is of an average of <strong>0.76%</strong> (so barely noticeable) with respect to the Sliding Window Attention version. In Fig.9, we can see (batch-wise) speedup in training:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/output%20(41)-480.webp 480w,/blog/assets/img/blog/beyond_attention/output%20(41)-800.webp 800w,/blog/assets/img/blog/beyond_attention/output%20(41)-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/output%20(41).png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig. 9: <em>speedup across steps of DSA and SWA vs baseline</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716-480.webp 480w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716-800.webp 800w,/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/beyond_attention/Pasted%20image%2020251016184716.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fig.10: <em>reported performance comparison between transformers and 2-simplicial attention in the original paper.</em></p> <h3 id="so-what-does-n-simplicial-attention-mean-for-depth">So what does $n$-simplicial attention mean for depth?</h3> <p>As we’ve discussed, one of the key elements of depth is <strong>multi-token representation-learning</strong>. Another way to view it, is that individual tokens are in a <strong>constant relay race</strong>: each token wants to get to a target representation, but needs crucial information from other tokens’ representations to do so. If the proper representation is very hard to find, the model eventually runs out of depth to message-pass. 2-simplicial attention goes in the direction of fixing this, because it <strong>combinatorially opens up surface area</strong> for the model to do message-passing, for each block. Of course, the present one is just its first, prototypal iteration, which will inevitably change in the future (us at <a href="https://gladia.netlify.app/">Gladia</a> are already hard at work).</p> <h3 id="wrapping-up">Wrapping up</h3> <p>We’ve explored a recent advance in attention architecture, and explained it using our previously established topologically-oriented angle. We’ve also outlined a trivial extension to n-simplices of the mechanism, as well as demonstrated tiny gains in expressivity by utilizing a DSA-like sparsification of 2-simplicial attention keys, substituting SWA. Given my obsession with the topic, you’re very likely to read something from me on the topic soon. In the meantime, let me know what you think!</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>Thanks a lot to <a href="https://x.com/thelokasiffers">thelakosiffers</a>, <a href="https://x.com/leothecurious">davinci</a> <a href="https://x.com/MatteoManias">MatteoManias</a>, <a href="https://x.com/f14bertolotti">Francesco Bertolotti</a>, <a href="https://x.com/_ueaj">ueaj</a>, <a href="https://x.com/biiordache">Bianca</a>, <a href="https://x.com/aurko79">Aurko</a>,<a href="https://x.com/_arohan_">Rohan</a> , <a href="https://x.com/mike64_t">Mike</a> , <a href="https://x.com/borak_004">Borak</a>, <a href="https://x.com/graffioh">Berto</a> and <a href="https://x.com/Niccolg92">Niccolò Gentile</a> for their precious feedback!</p> <hr/> <h3 id="suggested-citation">Suggested citation</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">pappone2025beyondattention</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Francesco Pappone}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Beyond Attention as  Graph}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="s">{October}</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{09}</span><span class="p">,</span>
  <span class="na">institution</span>  <span class="p">=</span> <span class="s">{Università La Sapienza di Roma -- PSTP Technoscience}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://publish.obsidian.md/the-tensor-throne/The+Graph+Side+of+Attention/Beyond+Attention+as+a+Graph}}</span><span class="p">,</span> 
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{Blogpost}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Francesco Pappone</name></author><category term="transformers"/><category term="attention"/><category term="topology"/><category term="higher-order attention"/><category term="simplicial attention"/><category term="The Graph Side of Attention series"/><summary type="html"><![CDATA[Higher-order (n-simplicial) attention as topology-driven message passing beyond graphs.]]></summary></entry><entry><title type="html">Attention sinks from the graph perspective</title><link href="https://crisostomi.github.io/blog/2025/attention_sinks/" rel="alternate" type="text/html" title="Attention sinks from the graph perspective"/><published>2025-08-17T00:00:00+00:00</published><updated>2025-08-17T00:00:00+00:00</updated><id>https://crisostomi.github.io/blog/2025/attention_sinks</id><content type="html" xml:base="https://crisostomi.github.io/blog/2025/attention_sinks/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51-480.webp 480w,/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51-800.webp 800w,/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Attention sinks have recently come back to the forefront of architecture discussion, especially due to their appearance in <a href="https://github.com/openai/gpt-oss">gpt-oss</a> (although in a different form than the effect we’re discussing today).</p> <p>As a mechanism, attention sinks are easy to describe: when trained, decoder-only transformer models tend to allocate a disproportionate amount of attention to the first few tokens, and especially to the first.</p> <p>This effect is well studied in its practical terms, and is often attributed to the model “offloading” probability mass to the early tokens to avoid their spurious allocation elsewhere. Recent works, like <a href="https://arxiv.org/abs/2504.20966">Softpick</a>, provide architectural choices that prevent sinks from forming. While this explanation may sound convincing at first glance, my intuition is still bothered by it: what do you mean the model “offloads”? Of course it doesn’t explore that possibility intentionally, there must be some mechanism by which the attention sinks are either advantageous or a result of an intrinsic bias in the model. In this blogpost, we will argue that there is a significant bias in decoder-only transformers that may be to blame, at least partially, for this phenomenon. Moreover, this will also allow us to introduce a series of blogposts focused on analyzing transformers from the lens of message passing on graphs.</p> <h2 id="attention-as-message-passing">Attention as message-passing</h2> <p><a href="https://arxiv.org/abs/2506.22084">Recent work by Chaitanya K. Joshi</a> has finally freed us from having to formalize independently a well known property of Transformers (and especially of attention layers): them being a special case of Graph Neural Networks (just like pretty much anything else, to be fair).</p> <p>As a setting to our discussion, though, we will go over another angle with which attention can be seen as message-passing on a graph.</p> <p>Most people are usually introduced to (multi-headed) self-attention directly via the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper. Despite this being generally a good practice in my opinion, it typically leads to attention being interpreted as the simplest way of making tokens interact in a transformer, or as just a soft version of a dictionary lookup. While neither view is wrong, such interpretations often drown out some interesting geometric details that lie in attention itself.</p> <p>Let’s start with regular, multi-headed attention.</p> <p>Say you have $n$ tokens, with an embedding dimension $d$.</p> <p>Let our input tokens be shaped as a matrix $X \in \mathbb{R}^{n \times d}$. We first process $X$ with three different linear projections, namely $W_q$, $W_k$ and $W_v$, and end up with the respective $Q \in \mathbb{R}^{n \times d_q}$, $K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ matrices.</p> <p>We then perform the well-known attention operation</p> \[\mathrm{attention}(X) = \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V.\] <p>Let’s take a look at $\alpha = QK^\top$. If we rewrite it component-wise we get</p> \[\alpha_{ij} = \sum_{\ell=1}^{d_k} Q_{i\ell} K_{j\ell}.\] <p>If we note that the rows of $Q$ and $K$ are respectively $q_i$ and $k_j$, we see that</p> \[\alpha_{ij} = q_i k_j^\top = \langle q_i, k_j \rangle.\] <p>The attention matrix $\alpha$’s entries are thus simply the Euclidean dot product between token embeddings, projected via the query and key matrices.</p> <p>This still falls within the classical presentation of attention, so nothing to see here as of yet.</p> <p>What if we could reinterpret these operations from a more geometric/topological perspective? Let’s take, for example,</p> \[X \in \mathbb{R}^{\,n\times d}.\] <p>And let’s treat the rows of $X$ as a <strong>point cloud</strong>:</p> \[X = \begin{bmatrix} x_1^\top\\ x_2^\top\\ \vdots\\ x_n^\top \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{\,n\times d_q},\] \[K = X W_k \in \mathbb{R}^{\,n\times d_k},\] \[V = X W_v \in \mathbb{R}^{\,n\times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^\top \in \mathbb{R}^{\,n\times n}.\] <p>The intuition is: the more points align in query-key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp\!\bigl(\alpha_{ij}/\sqrt{d_k}\bigr)} {\displaystyle\sum_{j'=1}^n \exp\!\bigl(\alpha_{ij'}/\sqrt{d_k}\bigr)}, \qquad A = \mathrm{softmax}\!\Bigl(\tfrac{\alpha}{\sqrt{d_k}}\Bigr).\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\mathrm{attention}(X) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong>.</p> <p>If we write it row-wise (hence focusing on each token, or node, at a time), we see that the updated function’s value associated with the node becomes</p> \[\mathrm{attention}(X)_i = \sum_\ell A_{i\ell} V_\ell.\] <p>But what does multiplying a function defined on a graph by the adjacency mean? Let’s say we have a directed graph $\mathcal{G} = (V,E)$ with adjacency $A$, with a function $f: v \rightarrow \mathbb{R}$ and $v \in V$. Then, the multiplication $y = Af$ can be written, component-wise, as</p> \[y_i = \sum_{j} A_{ij} f_j.\] <p>Remember that, for an adjacency matrix, elements of column $i$ represent incoming links from other nodes in the graph. This means that $y_i$, or the result of the adjacency-multiplied function $f$, is the weighted average of $f$ over incoming nodes to node $i$, where the weights are decided by the adjacency matrix entries. Intuitively, you can think of this process as a sort of <em>diffusion</em>: features are aggregates of their neighbours. This means that, if we start with a rather unequally spatially distributed function (say a very localized highly positive region, and the rest being zero), then nodes on the boundary of the highly positive region would “diffuse” the highly positive values towards neighbouring nodes. Of course the topology of the graph heavily influences the speed of this diffusion. Unsurprisingly, this ties back very well with the actual physical phenomenon of heat diffusion, as we will see in a future blogpost.</p> <h2 id="causal-transformers-and-attention-sinks">Causal Transformers and Attention Sinks</h2> <p>Note that the discussion so far has been agnostic of masking strategies applied to the attention score. While several uses of transformer models employ attention bidirectionally, LLMs, our large model protagonists, are usually causally masking attention to leverage parallelism for their next-token prediction task.</p> <p>In our attention mechanism, this is done by substituting our $\alpha$ adjacency matrix with a masked, causal one, in the shape of $\alpha_m = \alpha \odot M$, with $M_{ij} = 1$ if $j \leq i$ and zero otherwise. Note that this gives our attention graph an even more interesting structure: our graph is now, by design, a <strong>Directed Acyclic Graph</strong> (DAG), meaning the graph contains no loops, and its adjacency matrix is nilpotent (meaning there exists $k$ such that $(A^k)_{ij} = 0$, $\forall i,j$).</p> <p>One interesting corollary of this observation is that adjacency-based diffusion over DAGs is bound to accumulate information in sinks, specifically, in the first tokens of a causal model. This can be made explicit by looking at the shape of powers of $A$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E1-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E1-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E1.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E2-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E2-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E2.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E4-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E4-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E4.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E8-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E8-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E8.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These plots (Fig. 1-4) show exactly what we expect on a DAG: as we take powers of the (masked) attention matrix $A$ the mass moves “leftward” toward early tokens. In the strictly lower-triangular case (no self-loops) this is a nilpotent operator, so sufficiently high powers collapse entirely into the earliest positions.</p> <p>To connect this with learning dynamics, linearize one residual attention block (one head, for intuition; treat the MLP as a node-wise map) as</p> \[X^{\ell+1} \approx X^{\ell} + A^{\ell} X^{\ell} B^{\ell}, \qquad B^{\ell} = W_v^{\ell} W_o^{\ell}.\] <p>Stacking $L$ such blocks yields an end-to-end map that is a polynomial in the $A^{\ell}$’s:</p> \[X^{L} \approx \Big(\prod_{\ell=1}^{L} (I + A^{\ell} B^{\ell})\Big) X^{0} = X^{0} + \sum_{\ell} A^{\ell} B^{\ell} X^{0} + \sum_{\ell_2 &gt; \ell_1} A^{\ell_2} B^{\ell_2} A^{\ell_1} B^{\ell_1} X^{0} + \cdots\] <p>When the $A^{\ell}$ are geometrically similar across depth, dominant terms behave like <strong>powers of a causal $A$</strong>. That is the same “multi-hop diffusion” we saw in the previous figures, progressively concentrating influence onto the first columns (early tokens).</p> <p>But if that’s the case during a forward pass, what makes a model exhibit this bias across training, as it’s been noticed in the literature?</p> <p>As it turns out, backprop itself mirrors this geometry. Gradients w.r.t. hidden states propagate with Jacobian transposes along the value path:</p> \[g^{\ell} \approx (I + {B^{\ell+1}}^{\!\top} {A^{\ell+1}}^{\!\top}) \cdots (I + {B^{L}}^{\!\top} {A^{L}}^{\!\top}) g^{L}.\] <p>Hence token-wise gradients accumulate along <strong>column sums of products of $A$</strong> (or, equivalently, row sums of products of $A^{\top}$). In a causal DAG those column sums are largest for earlier positions, so both activations <strong>and</strong> gradients preferentially route through (and update) paths that point to early tokens.</p> <p>Practically, residual connections make the map a <strong>polynomial</strong> (not a single $A^k$), multi-head mixing and $B^{\ell}$ projections reshape directions, and layer-norm rescales signals. But the structural bias remains: deeper layers inherit updates that look like compositions of attention-diffusion steps, which, under causal masking, tend to be more and more “first-column concentrated”.</p> <p>Another corollary of our observation is that it would suggest that later layers are more subject to the attention sink phenomenon, while the very first layer should be much less impacted. This turns out to be true and well known when studying attention sinks, as is the case, for example, for <a href="https://arxiv.org/abs/2309.17453">Llama 2</a>, or in <a href="https://openreview.net/pdf/736acc55a9b7a936dff081c2ba066c205279a844.pdf">this paper</a> and <a href="https://arxiv.org/pdf/2402.09221">this one</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama-480.webp 480w,/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama-800.webp 800w,/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Note that, while this <strong>may not be the single effect responsible for attention sinks</strong>, this means we should expect any causal decoder-only transformer to exhibit a bias towards allocating attention to its first few tokens (and increasingly so to the first).</p> <p>This fundamentally clashes with many interpretations of sinks: several works characterize them as a useful feature that is learned by the model. If what we propose is true, it’s exactly the opposite: when sinks <strong>don’t</strong> show up, it means <strong>the message-passing mechanism of your transformer is fundamentally flawed</strong>, and hence it performs worse.</p> <p>The attention sinks become a signal of <strong>healthy communication</strong> of tokens in attention, being a bias that is <strong>intrinsic to the causal, decoder-only transformer</strong>.</p> <h2 id="wrapping-up">Wrapping up</h2> <p>So, to recap, what does this mean? We identified a possible mechanism that may bias causal transformers to accumulate attention on their first few tokens. Note that we showed the mechanism in a highly simplified setting, and are proposing the idea that, despite those simplifications, the underlying effect is still strong enough to accumulate across training steps of a large transformer, and eventually explain the existence of attention sinks as we know them. In the next blogposts, we will use the same graph-centric framing of attention to analyze the problem of long context in transformer models, connecting it to heat diffusion and the oversmoothing and oversquashing phenomena known in the GNN literature. Stay tuned!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Thanks a lot to <a href="https://x.com/thelakosiffers">thelakosiffers</a>, <a href="https://x.com/Niccolg92">Niccolò</a>, <a href="https://x.com/fabmilo">Fabrizio</a>, <a href="https://x.com/Cyndesama">Cynde</a>, <a href="https://x.com/f14bertolotti">Francesco</a> and <a href="https://x.com/zmkzmkz">Zed</a> for their precious feedback!</p> <hr/> <h2 id="suggested-citation">Suggested citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">pappone2025attentionsinks</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Francesco Pappone}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Attention sinks from the graph perspective}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="s">{August}</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">institution</span>  <span class="p">=</span> <span class="s">{Università La Sapienza di Roma -- PSTP Technoscience}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective}}</span><span class="p">,</span>
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{Blogpost}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Francesco Pappone</name></author><category term="transformers"/><category term="attention"/><category term="graph neural networks"/><category term="The Graph Side of Attention series"/><summary type="html"><![CDATA[Why causal transformers naturally concentrate attention on their earliest tokens.]]></summary></entry><entry><title type="html">Model Merging — a biased overview</title><link href="https://crisostomi.github.io/blog/2025/model_merging/" rel="alternate" type="text/html" title="Model Merging — a biased overview"/><published>2025-08-17T00:00:00+00:00</published><updated>2025-08-17T00:00:00+00:00</updated><id>https://crisostomi.github.io/blog/2025/model_merging</id><content type="html" xml:base="https://crisostomi.github.io/blog/2025/model_merging/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/weight_space-480.webp 480w,/blog/assets/img/blog/weight_space-800.webp 800w,/blog/assets/img/blog/weight_space-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/weight_space.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The HuggingFace Universe. Credits to the <a href="https://weight-space-learning.github.io/">Workshop on Weight Space Learning</a></figcaption> </figure> <aside class="notice"> <strong>Disclaimer</strong>: This is not a survey. It's more of a tour where my own work keeps getting suspiciously good seats. I promise I'll try a more balanced and comprehensive one in the future. </aside> <h2 id="motivation">Motivation</h2> <p>I recently attended an <a href="https://estimathon.com/">Estimathon</a> game. This is basically a quiz where you estimate for some hard-to-quantify question like <em>“How many cabs are there in New York?”</em><d-footnote>Around 12,000 <a href="https://en.wikipedia.org/wiki/Taxis_of_New_York_City">according to Wikipedia.</a></d-footnote> without using any tools. I was a total disaster. But let me ask you one Estimathon-style question I just made up:</p> <blockquote> <p><em>“How many models were there on HuggingFace one year ago?”</em></p> </blockquote> <p>Take a moment to think before scrolling. Got your answer? Nice. Click to reveal the answer.</p> <details><summary>Number of models on HuggingFace last year 👀</summary> <p>Answer: <strong>841,347</strong> models.</p> </details> <p>Were you close? No? Okay, another chance:</p> <blockquote> <p><em>“How many models are there on HuggingFace today?”</em></p> </blockquote> <p>As above, think about it. You’re basically guessing the growth rate of HuggingFace itself.</p> <details><summary>Number of models on HuggingFace today 👀</summary> <p>Answer: <strong>1,957,743</strong> models.</p> </details> <p>Almost got it this time? Cool, you win a t-shirt or something. As you can see, the number has <strong>more than doubled</strong> in just one year! <d-footnote>The number of models refers to the date of publication, i.e. August 18th 2025. Last year refers precisely to August 18th 2024, obtained via the <a href="https://web.archive.org/">WayBack Machine</a>.</d-footnote></p> <p>With this explosion of models, a natural question comes up: should we keep making new ones, or spend more effort reusing what we already have? If, like me, you lean toward the latter in many practical cases, this blogpost is for you. Apparently, that’s also the view at <em>Thinking Machines Lab</em> (which just closed a $2B seed round), where they plan to <em>“combine neural network layers from a range of open-source models with a technique that is similar to model merging”</em>.<d-footnote>See <a href="https://www.theinformation.com/articles/ex-openai-cto-muratis-startup-plans-compete-openai-others">The Information</a>. Sorry, paywalled!</d-footnote></p> <p>But even if you just want to make the GPUs go <em>brrr</em> and focus on training and tuning as much as possible, model merging might still be for you. ByteDance found merging effective for LLM pretraining <d-cite key="Yunshui2025-qz"></d-cite><d-footnote>See also <a href="https://x.com/giffmana/status/1924849877634449878">Lucas Beyer’s excellent summary</a>.</d-footnote>. Cohere’s latest CMD-A report backed that up. <d-cite key="cohere2025commandaenterprisereadylarge"></d-cite>.</p> <p>Enough motivation, let’s merge!</p> <hr/> <h2 id="intro-to-model-merging">Intro to model merging</h2> <p>First things first, what is <em>model merging</em>? Informally, it’s any technique that, given two or more models (the endpoints) and produces a new one that preserves their capabilities. Typically, we want the merging process to be data-free, and the resulting model to have:</p> <ol> <li>The same number of parameters as the endpoints.</li> <li>No extra runtime overhead.</li> </ol> <p>These requirements are not always enforced. Depending on the use case, it might make sense to relax one of them — for example, allowing <a href="#routing-and-moerging">extra overhead for routing</a> or using a small dataset to find <a href="#llms-and-evolutionary-merging">better merging coefficients</a>.</p> <p>Okay, but <em>what</em> models are we merging, and <em>why</em>? It’s helpful to think in terms of two broad categories. The subdivision is not strict, but it makes the landscape easier to navigate:</p> <ul> <li><strong>Merging models trained from scratch on the same task</strong> — covering linear mode connectivity, neuron permutation symmetries, and permutation matching.</li> <li><strong>Merging models finetuned from the same base model on different tasks</strong> — including task vectors, structure-aware merging methods, and evolutionary merging of LLMs.</li> </ul> <hr/> <h2 id="merging-models-trained-from-scratch-on-the-same-task">Merging models trained from scratch on the same task</h2> <p>The setup is simple: we start with two models initialized differently, $\theta^A_{\text{init}}$ and $\theta^B_{\text{init}}$. We train both on the same task $t$, then merge them into a new model $\theta_t^{A,B}$. Since the task index doesn’t add much information in this context, we drop it.</p> <h3 id="mode-connectivity">Mode connectivity</h3> <p>It all began with <strong>mode connectivity</strong>.</p> <p>People once thought that parameters corresponding to different minima (modes) were <strong>isolated</strong> in the loss landscape, each mode living in its own valley (<strong>basin</strong>). This was bad news: if true, you couldn’t move between minima without incurring a sharp loss increase. But can these modes actually be connected in weight space via a low-loss (or high-accuracy) path?</p> <aside> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/loss_landscape_plot-480.webp 480w,/blog/assets/img/blog/loss_landscape_plot-800.webp 800w,/blog/assets/img/blog/loss_landscape_plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/loss_landscape_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> Two modes in their basins. </p> </aside> <p>Turns out the answer is <strong>yes</strong>:</p> <ul> <li>Yes, and this allows us to interpolate between them to obtain cheap ensembles <d-cite key="Garipov2018-pz"></d-cite>.</li> <li>Yes, and in fact a large number of modes lie on a shared low-loss manifold <d-cite key="Draxler2018-vr"></d-cite>.</li> <li>Yes, and if the two modes share an initial training phase, the connecting path may even be linear <d-cite key="linear-mode-connectivity"></d-cite>.</li> </ul> <p>When the connecting path is linear, we usually assume the two modes to live in the same basin. This is kind of a big deal in model merging. Why? Well, because if the two modes can’t be connected linearly (i.e., linear interpolations between the modes result in a high loss), then we can’t average the models. Or, at least, can’t do it this simply. But how can we check if two modes are linearly connected, and to what degree? We will need to compute the loss barrier between the two.</p> \[\underbrace{\max_{\lambda \in [0,1]} \mathcal{L}\!\left((1-\lambda)\theta_A + \lambda \theta_B\right)}_{\text{highest loss along the path}} - \underbrace{\tfrac{1}{2}\Big(\mathcal{L}(\theta_A) + \mathcal{L}(\theta_B)\Big)}_{\text{average loss at endpoints}}\] <p>Here, the first term measures the highest loss encountered along the linear interpolation between the two modes, while the second one is the average loss at the endpoints. If the two modes lie in the same basin, the interpolated loss remains low and the barrier is close to zero. A high loss barrier instead suggests the presence of a peak or saddle point separating the two modes, indicating they belong to different regions of the loss landscape.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/loss_barrier-480.webp 480w,/blog/assets/img/blog/loss_barrier-800.webp 800w,/blog/assets/img/blog/loss_barrier-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/loss_barrier.png" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 400px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now that we know how to measure for closeness in the basin sense, we can move on to one of the key insights in model merging.</p> <hr/> <h3 id="neuron-permutation-symmetries">Neuron permutation symmetries</h3> <p>Consider a generic linear layer $z_{\ell+1} = \sigma(W_{\ell} z_{\ell})$, removing biases for simplicity. Now, what happens if we shuffle the rows of $W_{\ell}$? That’s equivalent to multiplying on the left by a permutation $P$, giving $W_\ell’ = P W_\ell$. The new output is</p> <aside> <p> Biases just need to be permuted along with the corresponding rows. </p> </aside> \[z'_{\ell+1} = \sigma(W_{\ell}' z_{\ell}) = \sigma(P W_{\ell} z_{\ell}) = P \sigma(W_{\ell} z_{\ell}) = P z_{\ell+1}.\] <p>Here, we’ve pulled $P$ outside $\sigma$, possible because nonlinearities act elementwise and commute with permutations.<d-footnote>Intuitively: whether you first shuffle a sequence and then apply an elementwise nonlinearity, or apply the nonlinearity first and then shuffle, the result is the same.</d-footnote></p> <p>So far, the two networks (with $W_{\ell}$ vs $W’_{\ell}$) aren’t the same; their outputs differ by a permutation. But let’s also permute the <strong>columns</strong> of the <em>next</em> layer by $P^\top$. Since permutation matrices are orthogonal ($P^{-1} = P^\top$), the output of the next layer becomes:</p> \[z'_{\ell+2} = \sigma(W_{\ell+1}' z'_{\ell+1}) = \sigma(W_{\ell+1} P^\top P z_{\ell+1}) = \sigma(W_{\ell+1} z_{\ell+1}) = z_{\ell+2}.\] <p>No tricks, the math checks out. The outputs are <strong>identical</strong>.</p> <p>So, are these two networks the same? Internally, no, their weights differ (potentially a lot). But functionally, yes: they compute the exact same mapping.</p> <p>This shows that <strong>neuron permutations create families of functionally equivalent networks.</strong> And since different random seeds lead to different permutations, many distinct-looking weight configurations are actually the <em>same function</em>.</p> <p>This led to the conjecture: once you account for permutations, all modes collapse into a single shared basin <d-cite key="Entezari2021-me"></d-cite>. If this was the case, then we should in principle be able to find, for each layer of model $A$, a permutation of its neurons that maps them to the neurons of the corresponding layer in model $B$, practically teleporting $A$ into $B$’s basin. There, <code class="language-plaintext highlighter-rouge">torch.mean(A, B)</code> is all you need!</p> <hr/> <h3 id="neuron-matching">Neuron matching</h3> <p>We wrapped up the previous section looking for a magical way to align the neurons of two models. One possible objective could be the following</p> \[\arg\max_{\{P_\ell \in \mathbb{P}\}} \sum_{\ell=1}^{L} \left\langle W_\ell^A, \, P_\ell W_\ell^B P_{\ell-1}^{\top} \right\rangle\] <p>where</p> \[\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i=1}^m \langle A_{i,:}, B_{i,:} \rangle,\] <p>so at layer $\ell$ we are basically looking for the permutation $P_{\ell}$ that best aligns the rows (neurons) of the two matrices in a dot product sense. $P_{\ell}$ is then also applied, after transposition, to the subsequent layer to maintain functional equivalence.</p> <aside> <p>Remember we are searching in the space of functionally equivalent networks.</p> </aside> <p>Like all good things in life, this problem is NP-Hard <d-cite key="git-rebasin"></d-cite>. The Git Re-Basin way, at this point, is to do a layer-wise approximation of the problem, in which the previous and subsequent permutations are held fixed, making each problem a simple Linear Assignment Problem (LAP).<d-footnote>Such a problem allows efficient off-the-shelf algorithms like the <a href="https://en.wikipedia.org/wiki/Hungarian_algorithm">Hungarian algorithm</a>.</d-footnote> Find a (somewhat simplified) version of the algorithm below.</p> <d-code block="" language="python"> <pre><code class="language-python">

def match_neurons(A, B):
    # Initialize permutations as identity
    P = [identity(N) for l in layers]

    for i in num_steps:
        progress = False
        for l in shuffle(layers):

            W_A, W_B = A[l], B[l]

            # Compute neuron-to-neuron similarity matrix under current permutation
            sim = compute_similarity(W_A, P[l] @ W_B @ P[l-1])

            # Solve linear assignment problem
            new_P_l = linear_assignment_problem(sim)

            # Update permutation
            P[l] = new_P_l

            # Recompute similarity
            sim_new = compute_similarity(W_A, P[l] @ W_B @ P[l-1])

            if sim_new.mean() &gt; sim.mean():
                progress = True

        if not progress:
            return P
</code></pre> </d-code> <p>Surprisingly, while Git Re-Basin (2022) is probably the most popular matching algorithm, a precursor (and also, a general case) of this algorithm by S.P. Singh and M. Jaggi was already around in 2019! <d-cite key="singh2020model"></d-cite> There, permutations are replaced by soft maps. These generalize permutations, since a soft map is just a doubly stochastic matrix. For instance a permutation matrix between two sets of $3$ objects would look like</p> \[\textbf{Permutation matrix } P = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\] <p>so each row/column has exactly one 1 (hard assignment), while a soft map may look like</p> \[\textbf{Soft map } S = \begin{bmatrix} 0.7 &amp; 0.3 &amp; 0.0 \\ 0.2 &amp; 0.5 &amp; 0.3 \\ 0.1 &amp; 0.2 &amp; 0.7 \end{bmatrix}\] <p>so rows/columns sum to 1, entries in $[0,1]$ (fractional assignment). Soft maps are the bread and butter of optimal transport.</p> <hr/> <h3 id="entering-cycle-consistency">Entering cycle-consistency</h3> <p>Now entering my own work: I’ll try to keep it fair and high level, so I don’t monopolize the overview. I can do this. The question here is: what happens if we want to match and merge more than 2 models? The easiest solution would be to choose one reference model, find pairwise maps to it, and then aggregate everything in this one. Couple of problems: when optimizing for each pair, the optimization is not aware of the other models. This means that the maps do not compose gracefully: if one maps from a model A to B, then to C and back to A, the end result would be way different than the starting point. In other words, the maps <strong>lack cycle-consistency</strong>. The other (main) problem is that the reference model is an arbitrary choice. And we know, arbitrariness is never good as it opens the way to variance in the results: depending on a (often random) choice might change the results by double digits. Let’s try something else! In $C^2M^3$ <d-cite key="cycle-consistent"></d-cite> we start from the <a href="#neuron-matching">standard weight matching equation</a> and consider all the possible pairs of models</p> \[\arg\max_{\{P_i^{pq} \in \mathbb{P}\}} \sum_{\substack{(p,q)}} \;\; \sum_{i=1}^{L} \left\langle W_i^p,\; P_i^{pq} W_i^q \left(P_{i-1}^{pq}\right)^{\top} \right\rangle\] <p>We factorize each permutation $P^{BA}$ (mapping $A$ to $B$) as the composition of two permutations: one mapping $A$ to a universe space, and one mapping from the universe back to $B$. Since each time you have to pass through the universe, you end-up with a series of permutations that cancel each other out, eventually composing to the identity and guaranteeing cycle-consistency.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/pairwise-vs-cyclecons-480.webp 480w,/blog/assets/img/blog/pairwise-vs-cyclecons-800.webp 800w,/blog/assets/img/blog/pairwise-vs-cyclecons-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/pairwise-vs-cyclecons.png" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The optimization problem we obtain is similar, just a tiny bit uglier due to the factorizations. We optimize this one with Frank-Wolfe<d-footnote>Frank-Wolfe considers a linear approximation of the objective function, and moves towards a minimizer of this linear function. Works for problems with convex solution sets and convex objective functions.</d-footnote>, check the paper for the full details! For brevity, let’s just say merging in the universe space works very well: modes are much more connected there than they originally were!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/basins_after_mapping.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/basins_after_mapping.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="merging-models-finetuned-from-the-same-base-model-on-different-tasks">Merging models finetuned from the same base model on different tasks</h2> <p>We now switch setup. Tabula rasa. This time we start from a common pretrained model $\theta_{\text{pt}}$, which is finetuned separately on different tasks $t_1, t_2$ (say, MNIST and CIFAR-100) to obtain $\theta_{t_1}$ and $\theta_{t_2}$. Our goal is to combine them into a single model $\theta_{t_1,t_2}$ that:</p> <aside>Notation note: I sometimes use $\theta_{\text{pt}}$ or $\theta_{\text{base}}$ to mean the same thing: the pretrained foundation model before finetuning.</aside> <ul> <li>Has the same number of parameters as the base.</li> <li>Can do both tasks at once.</li> </ul> <p>As before, we want this merging process to be data-free. Picture downloading two finetuned checkpoints from HuggingFace and fusing them into a single multi-task model without touching the original datasets.</p> <aside>You often don't even have access to finetuning datasets.</aside> <p>So how do we do it?</p> <h3 id="task-arithmetic">Task arithmetic</h3> <p>The story begins with a simple yet powerful observation: finetuning on a task looks like <em>adding a vector</em> in weight space. This is trivially true when you consider the difference between the finetuned model and its base. Let us denote the update induced by task $t$ as the <strong>task vector</strong></p> \[\tau_t = \theta_t - \theta_{\text{pt}}.\] <p>Then, finetuning is nothing more than</p> \[\theta_t = \theta_{\text{pt}} + \tau_t.\] <p>This way of writing things suggests an almost irresistible idea: if we want a model that can do both $t_1$ and $t_2$, why not just add their updates?</p> \[\theta_{t_1,t_2} = \theta_{\text{pt}} + \tau_{t_1} + \tau_{t_2}.\] <p>And there you have it: task arithmetic <d-cite key="task-vectors"></d-cite>.</p> <p>Of course, life isn’t quite that easy. Sometimes adding updates works beautifully, sometimes it leads to catastrophic interference, and sometimes you get a weird in-between model that can’t do either of the tasks. Still, the arithmetic view was an important first step: it made explicit that task updates behave <em>vectorially</em>, and therefore can be combined, compared, and even algebraically manipulated.</p> <p>So, in general, merging models through task arithmetic boils down to</p> \[\theta_{\text{MT}} = \theta_{\text{pt}} + \alpha \sum_{t \in T} \tau_t\] <p>where $\alpha$ is a scaling factor that is usually optimized on a validation set.</p> <h3 id="task-vectors-and-gradients">Task vectors and gradients</h3> <p>When I first heard about task arithmetic, I was puzzled: why should adding two random finetuning updates give anything meaningful? Let’s do a simple thought experiment.</p> <p>Suppose you finetune a pretrained model with vanilla gradient descent, no minibatches, and only a <em>single epoch</em>. In this case, the task vector is exactly the negative gradient of the loss at the base model:</p> \[\tau_t = - \eta \, \nabla \overline{\mathcal{L}}_t(\theta_{\text{pt}}),\] <p>where $\eta$ is the learning rate and $\overline{\mathcal{L}}_t$ denotes the average loss over task $t$.</p> <p>Now, if we add task vectors from multiple tasks, linearity of the gradient gives</p> \[\sum_{t \in T} \tau_t = \sum_{t \in T} \Big(- \eta \, \nabla \overline{\mathcal{L}}_t(\theta_{\text{pt}})\Big) = - \eta \, \nabla \Bigg(\sum_{t \in T} \overline{\mathcal{L}}_t\Bigg) = - \eta \, \nabla \overline{\mathcal{L}}_T(\theta_{\text{pt}}).\] <p>In other words, adding task vectors is (in this restricted setting) equivalent to taking a single <strong>multi-task gradient step</strong> from the pretrained model!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/task_vector_vs_gradient-480.webp 480w,/blog/assets/img/blog/task_vector_vs_gradient-800.webp 800w,/blog/assets/img/blog/task_vector_vs_gradient-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/task_vector_vs_gradient.png" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>What about the scaling factor $\alpha$ we introduced earlier? In this view, $\alpha$ simply plays the role of the learning rate: tuning it adjusts how far you move along the combined gradient.</p> <p>Of course, this equivalence only holds in a very idealized setting (no minibatches, one pass, no curvature). With minibatches or multiple epochs, the connection weakens. Still, this perspective grounds task arithmetic in optimization, moving it away from magic vector algebra and toward something more principled. In <d-cite key="zhou2025taskvectors"></d-cite> we show to what degree this relation holds in practice, check it out!</p> <h3 id="structure-aware-merging-methods">Structure-aware merging methods</h3> <p>So up until now we have considered task vectors as flat vectors, so these $\theta$s live in $\mathbb{R}^n$, and so does $\tau$</p> \[\theta_{\text{MT}} = \theta_{\text{pt}} + \alpha \sum_{t \in T} \tau_t\] <p>But we all know that $\theta$ are not really flat vectors right? And neither should be $\tau$, since at some layers it may have a matrix or tensor structure.</p> <p>So we first consider these differences layer-wise, defining layer-wise task matrices instead of the global task vectors we’ve discussed so far.</p> \[\theta_{\text{MT}}^{(l)} = \theta_{\text{pt}}^{(l)} + \alpha \sum_{t \in T} \Delta_t^{(l)}\] <p>And ask ourselves, can we actually leverage this structure? As it is often the case, we will start by studying the SVD of layers having a matrix structure.</p> \[\Delta = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \, u_i v_i^\top\] <p>where $r$ is the rank of the matrix. The next question follows naturally: are $\Delta$s low-rank? i.e., we try to approximate $\Delta$ with</p> \[\tilde{\Delta} = \tilde{U} \tilde{\Sigma} \tilde{V}^\top = \sum_{i=1}^{k} \sigma_i \, u_i v_i^\top \quad k \ll r.\] <p>Of course they are!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/deltas-low-rank.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/deltas-low-rank.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Ok cool then I guess we can just sum the low-rank approximations $\tilde{\Delta}$ and solve merging… right? Not so fast. Check this out.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/similarity_matrices_sing_vectors.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/similarity_matrices_sing_vectors.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 300px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Basically there is a strong interplay (measured as the dot product) between the singular vectors from different tasks, and this induces interference in the merging. What we do then in Task Singular Vectors <d-cite key="TSV"></d-cite>, is we orthogonalize these across tasks before merging through Procrustes orthogonalization.<d-footnote>Don't use Gram-Schmidt, we wasted months on that!</d-footnote> The results you get this way were fairly hard to believe at first.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/radar-charts.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/radar-charts.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 600px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="routing-and-moerging">Routing and MoErging</h3> <p>Wrapping up Task Singular Vectors <d-cite key="TSV"></d-cite>, the final merged update is given by</p> \[\Delta_{\text{MT}} = \sum_{t \in T} \sum_{i=1}^{k} \sigma_i \, u_i v_i^\top\] <p>Since the merging process is a one-time, data-free step, there is not much we can do at this point. But what if we relax one of our original assumptions and instead allow some extra inference-time compute? Intuitively, it would be great if we could select only the singular vectors for the right task</p> \[\Delta_{\mathrm{MT}}=\sum_{i=1}^{T} \underset{[i=h]}{\mathbb{1}} \sum_{j=1}^k \sigma_j^i u_j^i v_j^{i \top}=\sum_{j=1}^k \sigma_j^h u_j^h v_j^{h \top}=\hat{\Delta}_h\] <p>as this would be equivalent to using the low-rank approximated $\tilde{\Delta}$, which we’ve seen preserves something like $99.5\%$ of the accuracy! Even if we could just restrict the set of selected tasks to $K$ instead of $N$ it would be something. Now, I know what you’re thinking.. a router. But didn’t we want to be data-free? turns out we can <d-cite key="mass"></d-cite>.</p> <aside> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/projection.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/projection.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </aside> <p>Say that we embed our sample $x$ with some model (e.g. one we previously merged) to obtain $z_\ell$. What we can do now is to compute the residual obtained from its projection onto each task-specific subspace as spanned by the corresponding singular vectors $V_t$</p> \[r_t \gets \| z_\ell - V_t V_t^\top z_\ell \|_2.\] <p>This gives us a vector of unnormalized logits, which (surprise surprise) we can use to compute a softmax over the tasks to get the distribution we wanted. Optionally, we can threshold the resulting probabilities and choose a maximum $K$ defining the number of tasks that can be selected. We can now only merge these ones and use the resulting model for inference!</p> <p>Why should the right singular vectors of the $\Delta$s be a proper choice for task classification? It works very well in practice, and we show some cool experiments in the paper. I’ll try to write a blogpost about it soon.</p> <h3 id="llms-and-evolutionary-merging">LLMs and Evolutionary Merging</h3> <p>Ok this all sounds cool, but what about LLMs? Well, it turns out that for these ones, task arithmetic already works quite well. There are some slightly more sophisticated methods which I will hopefully write about when I expand the blog, but for the moment let’s just say that they don’t usually result in double-digit improvements.</p> <p>Things change, however, if we drop the data-free constraint. In this case, results can be significantly better. One notable example is a recent paper from Sakana AI, proposing to use evolutionary algorithms to search for the best combination coefficients <d-cite key="akiba2025evolutionary"> </d-cite>. The results are quite impressive: they manage to synthesize a new Japanese LLM with state-of-the-art math-solving skills by merging a Japanese Mistral-7B finetuning with some math-specific finetunings.<d-footnote>Clearly, state-of-the-art in answering math questions in Japanese; merging is cool but it doesn't create new knowledge out of thin air.</d-footnote></p> <p>Evolutionary algorithms are cool, but they are not particularly famous for their efficiency. Just think that the framework involves a loop like the following</p> <d-code block="" language="python"> <pre><code class="language-python">

for step in steps:
  for model in population:
    fitness = eval(model, dataset)

</code></pre> </d-code> <p>where <code class="language-plaintext highlighter-rouge">eval</code> is a function that evaluates each model (LLM with a gazillion parameters) on the dataset and returns a fitness score. Each call might take hours. Multiply those hours by the number of models in each population and the number of steps, and you get a pretty good idea of the total compute cost involved. Merging is a pretty democratic tool, with hundreds of thousands of models being merged and uploaded on HuggingFace by everyday users on consumer GPUs, or even without accelerated hardware. So we ask ourselves, can we preserve some of the end performance of evolutionary merging but still allow common users to use it? The answer, against all expectations by the reader, is yes. Enter MERGE$^3$ <d-cite key="mencattini2025merge"></d-cite>. Intuitively, if we had a way to significantly shrink the evaluation dataset, we could make this process much more efficient. What if we had a way to estimate the accuracy of a model over a whole dataset given just a few of its samples? Say, 20? That’s precisely the goal of <em>Item Response Theory</em> (IRT). To do this, we use the logistic model proposed by tinyBenchmarks <d-cite key="polotinybenchmarks"></d-cite>. With this framework, we can approximate the model’s accuracy as the probability that the model is correct averaged over all the samples in the dataset</p> \[\operatorname{accuracy}(m) \approx \frac{1}{N} \sum_{i=1}^N \mathbb{P}(Y_{im} = 1 \, | \, {\color{OliveGreen}\gamma_m}, {\color{YellowOrange}\alpha_i}, {\color{Cyan}\beta_i}) = \frac{1}{N} \sum_{i=1}^N \frac{1}{1 + \exp(- {\color{YellowOrange}\alpha_i}^\top {\color{OliveGreen}\gamma_m} + {\color{Cyan}\beta_i})}.\] <p>This probability depends upon three sets of parameters, $\alpha$s, $\beta$s and $\gamma$s. For a model $m$, ${\color{OliveGreen}\gamma_m}$ encodes its latent ability, while ${\color{YellowOrange}\alpha_i}$ selects what abilities are required for a particular sample. So the higher the alignment between ${\color{YellowOrange}\alpha}$ and ${\color{OliveGreen}\gamma}$, the higher the probability. The difficulty parameter ${\color{Cyan}\beta_i}$ then acts as a kind of per-sample threshold that shifts the center of the probability, i.e. the value that your alignment must have to reach a probability of 0.5. Without it, this would be just a standard logistic function, with probability 0.5 at logits equal to 0.</p> <p>Now, ${\color{YellowOrange}\alpha}$s and ${\color{Cyan}\beta}$s can be precomputed robustly over the dataset with different models, but ${\color{OliveGreen}\gamma}$ is model-specific. During the evolution we therefore have to estimate ${\color{OliveGreen}\gamma}$ only on the small subsample. Our key insight is that, being the model at hand a combination of the endpoints, also its abilities will be some combination of those of the endpoints. In particular, we assume this combination to be linear, and instead of fitting ${\color{OliveGreen}\gamma}$ directly, we fit its interpolation coefficients</p> \[{\color{OliveGreen}\gamma_{\tilde{m}}} = \sum_{j=1}^m \lambda_j \,{\color{OliveGreen}\gamma_{j}}\] <p>When we plug this new estimator into the evolution pipeline, we obtain results close to those obtained with the full dataset, but at a fraction of the computational cost. Indeed, the experiments that follow were run on an Nvidia 4090 in just a day!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/merge3_results.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/merge3_results.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="what-comes-next">What comes next?</h2> <p>Looking at the big picture, the motivations for merging diverge a bit across domains. In computer vision, the main driver has been compression: taking multiple models trained on the same or related tasks and recycling their parameters into a more compact form. In language, by contrast, merging is mostly about compositionality: fusing different finetunings so the result can do a task that neither of the endpoints could do. Put simply: in vision, it’s fine if the merged model is just the sum of its parts; in language, we often hope for something greater than the sum.</p> <p>A natural, unsurprising goal for compression-oriented model merging is to reach the multi-task upper bound. Multi-task learning still suffers from task interference, but backpropagation at least helps mitigate it.</p> <p>Beyond that, some frontiers are still wide open. Heterogeneous merging (combining models with different architectures, widths, or parameter counts) remains largely unsolved. Most existing approaches assume architectural homogeneity, with the occasional exception of optimal-transport–based methods that are less rigid. Cracking this problem would unlock unprecedented reuse.</p> <p>Finally, we’re still mostly blind when it comes to understanding why merging works. In some cases, merges succeed spectacularly; in others, they fail badly. Could tools from mechanistic interpretability come handy here?</p> <p>I’ll try to continuously improve this blogpost, but for today that’s about as far as I can take you. Until next time, happy merging!</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>Of course, when I say my work, I really mean the collective effort of many brilliant collaborators<d-footnote>Listed in alphabetical order by last name.</d-footnote>: Daniele Baieri, Florian Bernard, Maria Sofia Bucarelli, Giuseppe Alessio D’Inverno, Marco Fumero, Antonio Andrea Gargiulo, Iacopo Masi, Tommaso Mencattini, Robert Adrian Minut, Emanuele Rodolà, Andrea Santilli, Simone Scardapane, Fabrizio Silvestri, Daniele Solombrino, Luca Zhou, Alessandro Zirilli.</p> <p>Thank you!</p>]]></content><author><name>Donato Crisostomi</name></author><category term="model merging"/><category term="machine learning"/><category term="research"/><summary type="html"><![CDATA[A friendly tour of model merging, suspiciously aligned with my own research.]]></summary></entry></feed>