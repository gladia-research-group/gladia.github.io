<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://crisostomi.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://crisostomi.github.io/blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-01T17:29:32+00:00</updated><id>https://crisostomi.github.io/blog/feed.xml</id><title type="html">GLADIA</title><subtitle></subtitle><entry><title type="html">Attention sinks from the graph perspective</title><link href="https://crisostomi.github.io/blog/2025/attention_sinks/" rel="alternate" type="text/html" title="Attention sinks from the graph perspective"/><published>2025-08-17T00:00:00+00:00</published><updated>2025-08-17T00:00:00+00:00</updated><id>https://crisostomi.github.io/blog/2025/attention_sinks</id><content type="html" xml:base="https://crisostomi.github.io/blog/2025/attention_sinks/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51-480.webp 480w,/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51-800.webp 800w,/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/ChatGPT%20Image%2024%20ago%202025,%2016_42_51.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Attention sinks have recently come back to the forefront of architecture discussion, especially due to their appearance in <a href="https://github.com/openai/gpt-oss">gpt-oss</a> (although in a different form than the effect we’re discussing today).</p> <p>As a mechanism, attention sinks are easy to describe: when trained, decoder-only transformer models tend to allocate a disproportionate amount of attention to the first few tokens, and especially to the first.</p> <p>This effect is well studied in its practical terms, and is often attributed to the model “offloading” probability mass to the early tokens to avoid their spurious allocation elsewhere. Recent works, like <a href="https://arxiv.org/abs/2504.20966">Softpick</a>, provide architectural choices that prevent sinks from forming. While this explanation may sound convincing at first glance, my intuition is still bothered by it: what do you mean the model “offloads”? Of course it doesn’t explore that possibility intentionally, there must be some mechanism by which the attention sinks are either advantageous or a result of an intrinsic bias in the model. In this blogpost, we will argue that there is a significant bias in decoder-only transformers that may be to blame, at least partially, for this phenomenon. Moreover, this will also allow us to introduce a series of blogposts focused on analyzing transformers from the lens of message passing on graphs.</p> <h2 id="attention-as-message-passing">Attention as message-passing</h2> <p><a href="https://arxiv.org/abs/2506.22084">Recent work by Chaitanya K. Joshi</a> has finally freed us from having to formalize independently a well known property of Transformers (and especially of attention layers): them being a special case of Graph Neural Networks (just like pretty much anything else, to be fair).</p> <p>As a setting to our discussion, though, we will go over another angle with which attention can be seen as message-passing on a graph.</p> <p>Most people are usually introduced to (multi-headed) self-attention directly via the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper. Despite this being generally a good practice in my opinion, it typically leads to attention being interpreted as the simplest way of making tokens interact in a transformer, or as just a soft version of a dictionary lookup. While neither view is wrong, such interpretations often drown out some interesting geometric details that lie in attention itself.</p> <p>Let’s start with regular, multi-headed attention.</p> <p>Say you have $n$ tokens, with an embedding dimension $d$.</p> <p>Let our input tokens be shaped as a matrix $X \in \mathbb{R}^{n \times d}$. We first process $X$ with three different linear projections, namely $W_q$, $W_k$ and $W_v$, and end up with the respective $Q \in \mathbb{R}^{n \times d_q}$, $K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ matrices.</p> <p>We then perform the well-known attention operation</p> \[\mathrm{attention}(X) = \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V.\] <p>Let’s take a look at $\alpha = QK^\top$. If we rewrite it component-wise we get</p> \[\alpha_{ij} = \sum_{\ell=1}^{d_k} Q_{i\ell} K_{j\ell}.\] <p>If we note that the rows of $Q$ and $K$ are respectively $q_i$ and $k_j$, we see that</p> \[\alpha_{ij} = q_i k_j^\top = \langle q_i, k_j \rangle.\] <p>The attention matrix $\alpha$’s entries are thus simply the Euclidean dot product between token embeddings, projected via the query and key matrices.</p> <p>This still falls within the classical presentation of attention, so nothing to see here as of yet.</p> <p>What if we could reinterpret these operations from a more geometric/topological perspective? Let’s take, for example,</p> \[X \in \mathbb{R}^{\,n\times d}.\] <p>And let’s treat the rows of $X$ as a <strong>point cloud</strong>:</p> \[X = \begin{bmatrix} x_1^\top\\ x_2^\top\\ \vdots\\ x_n^\top \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{\,n\times d_q},\] \[K = X W_k \in \mathbb{R}^{\,n\times d_k},\] \[V = X W_v \in \mathbb{R}^{\,n\times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^\top \in \mathbb{R}^{\,n\times n}.\] <p>The intuition is: the more points align in query-key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp\!\bigl(\alpha_{ij}/\sqrt{d_k}\bigr)} {\displaystyle\sum_{j'=1}^n \exp\!\bigl(\alpha_{ij'}/\sqrt{d_k}\bigr)}, \qquad A = \mathrm{softmax}\!\Bigl(\tfrac{\alpha}{\sqrt{d_k}}\Bigr).\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\mathrm{attention}(X) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong>.</p> <p>If we write it row-wise (hence focusing on each token, or node, at a time), we see that the updated function’s value associated with the node becomes</p> \[\mathrm{attention}(X)_i = \sum_\ell A_{i\ell} V_\ell.\] <p>But what does multiplying a function defined on a graph by the adjacency mean? Let’s say we have a directed graph $\mathcal{G} = (V,E)$ with adjacency $A$, with a function $f: v \rightarrow \mathbb{R}$ and $v \in V$. Then, the multiplication $y = Af$ can be written, component-wise, as</p> \[y_i = \sum_{j} A_{ij} f_j.\] <p>Remember that, for an adjacency matrix, elements of column $i$ represent incoming links from other nodes in the graph. This means that $y_i$, or the result of the adjacency-multiplied function $f$, is the weighted average of $f$ over incoming nodes to node $i$, where the weights are decided by the adjacency matrix entries. Intuitively, you can think of this process as a sort of <em>diffusion</em>: features are aggregates of their neighbours. This means that, if we start with a rather unequally spatially distributed function (say a very localized highly positive region, and the rest being zero), then nodes on the boundary of the highly positive region would “diffuse” the highly positive values towards neighbouring nodes. Of course the topology of the graph heavily influences the speed of this diffusion. Unsurprisingly, this ties back very well with the actual physical phenomenon of heat diffusion, as we will see in a future blogpost.</p> <h2 id="causal-transformers-and-attention-sinks">Causal Transformers and Attention Sinks</h2> <p>Note that the discussion so far has been agnostic of masking strategies applied to the attention score. While several uses of transformer models employ attention bidirectionally, LLMs, our large model protagonists, are usually causally masking attention to leverage parallelism for their next-token prediction task.</p> <p>In our attention mechanism, this is done by substituting our $\alpha$ adjacency matrix with a masked, causal one, in the shape of $\alpha_m = \alpha \odot M$, with $M_{ij} = 1$ if $j \leq i$ and zero otherwise. Note that this gives our attention graph an even more interesting structure: our graph is now, by design, a <strong>Directed Acyclic Graph</strong> (DAG), meaning the graph contains no loops, and its adjacency matrix is nilpotent (meaning there exists $k$ such that $(A^k)_{ij} = 0$, $\forall i,j$).</p> <p>One interesting corollary of this observation is that adjacency-based diffusion over DAGs is bound to accumulate information in sinks, specifically, in the first tokens of a causal model. This can be made explicit by looking at the shape of powers of $A$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E1-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E1-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E1.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E2-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E2-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E2.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E4-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E4-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E4.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/new_A%5E8-480.webp 480w,/blog/assets/img/blog/attention_sinks/new_A%5E8-800.webp 800w,/blog/assets/img/blog/attention_sinks/new_A%5E8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/new_A%5E8.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These plots (Fig. 1-4) show exactly what we expect on a DAG: as we take powers of the (masked) attention matrix $A$ the mass moves “leftward” toward early tokens. In the strictly lower-triangular case (no self-loops) this is a nilpotent operator, so sufficiently high powers collapse entirely into the earliest positions.</p> <p>To connect this with learning dynamics, linearize one residual attention block (one head, for intuition; treat the MLP as a node-wise map) as</p> \[X^{\ell+1} \approx X^{\ell} + A^{\ell} X^{\ell} B^{\ell}, \qquad B^{\ell} = W_v^{\ell} W_o^{\ell}.\] <p>Stacking $L$ such blocks yields an end-to-end map that is a polynomial in the $A^{\ell}$’s:</p> \[X^{L} \approx \Big(\prod_{\ell=1}^{L} (I + A^{\ell} B^{\ell})\Big) X^{0} = X^{0} + \sum_{\ell} A^{\ell} B^{\ell} X^{0} + \sum_{\ell_2 &gt; \ell_1} A^{\ell_2} B^{\ell_2} A^{\ell_1} B^{\ell_1} X^{0} + \cdots\] <p>When the $A^{\ell}$ are geometrically similar across depth, dominant terms behave like <strong>powers of a causal $A$</strong>. That is the same “multi-hop diffusion” we saw in the previous figures, progressively concentrating influence onto the first columns (early tokens).</p> <p>But if that’s the case during a forward pass, what makes a model exhibit this bias across training, as it’s been noticed in the literature?</p> <p>As it turns out, backprop itself mirrors this geometry. Gradients w.r.t. hidden states propagate with Jacobian transposes along the value path:</p> \[g^{\ell} \approx (I + {B^{\ell+1}}^{\!\top} {A^{\ell+1}}^{\!\top}) \cdots (I + {B^{L}}^{\!\top} {A^{L}}^{\!\top}) g^{L}.\] <p>Hence token-wise gradients accumulate along <strong>column sums of products of $A$</strong> (or, equivalently, row sums of products of $A^{\top}$). In a causal DAG those column sums are largest for earlier positions, so both activations <strong>and</strong> gradients preferentially route through (and update) paths that point to early tokens.</p> <p>Practically, residual connections make the map a <strong>polynomial</strong> (not a single $A^k$), multi-head mixing and $B^{\ell}$ projections reshape directions, and layer-norm rescales signals. But the structural bias remains: deeper layers inherit updates that look like compositions of attention-diffusion steps, which, under causal masking, tend to be more and more “first-column concentrated”.</p> <p>Another corollary of our observation is that it would suggest that later layers are more subject to the attention sink phenomenon, while the very first layer should be much less impacted. This turns out to be true and well known when studying attention sinks, as is the case, for example, for <a href="https://arxiv.org/abs/2309.17453">Llama 2</a>, or in <a href="https://openreview.net/pdf/736acc55a9b7a936dff081c2ba066c205279a844.pdf">this paper</a> and <a href="https://arxiv.org/pdf/2402.09221">this one</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama-480.webp 480w,/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama-800.webp 800w,/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/attention_sinks/attention_sinks_in_llama.png" class="img-fluid rounded" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Note that, while this <strong>may not be the single effect responsible for attention sinks</strong>, this means we should expect any causal decoder-only transformer to exhibit a bias towards allocating attention to its first few tokens (and increasingly so to the first).</p> <p>This fundamentally clashes with many interpretations of sinks: several works characterize them as a useful feature that is learned by the model. If what we propose is true, it’s exactly the opposite: when sinks <strong>don’t</strong> show up, it means <strong>the message-passing mechanism of your transformer is fundamentally flawed</strong>, and hence it performs worse.</p> <p>The attention sinks become a signal of <strong>healthy communication</strong> of tokens in attention, being a bias that is <strong>intrinsic to the causal, decoder-only transformer</strong>.</p> <h2 id="wrapping-up">Wrapping up</h2> <p>So, to recap, what does this mean? We identified a possible mechanism that may bias causal transformers to accumulate attention on their first few tokens. Note that we showed the mechanism in a highly simplified setting, and are proposing the idea that, despite those simplifications, the underlying effect is still strong enough to accumulate across training steps of a large transformer, and eventually explain the existence of attention sinks as we know them. In the next blogposts, we will use the same graph-centric framing of attention to analyze the problem of long context in transformer models, connecting it to heat diffusion and the oversmoothing and oversquashing phenomena known in the GNN literature. Stay tuned!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Thanks a lot to <a href="https://x.com/thelakosiffers">thelakosiffers</a>, <a href="https://x.com/Niccolg92">Niccolò</a>, <a href="https://x.com/fabmilo">Fabrizio</a>, <a href="https://x.com/Cyndesama">Cynde</a>, <a href="https://x.com/f14bertolotti">Francesco</a> and <a href="https://x.com/zmkzmkz">Zed</a> for their precious feedback!</p> <hr/> <h2 id="suggested-citation">Suggested citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">pappone2025attentionsinks</span><span class="p">,</span>
  <span class="na">author</span>       <span class="p">=</span> <span class="s">{Francesco Pappone}</span><span class="p">,</span>
  <span class="na">title</span>        <span class="p">=</span> <span class="s">{Attention sinks from the graph perspective}</span><span class="p">,</span>
  <span class="na">year</span>         <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>        <span class="p">=</span> <span class="s">{August}</span><span class="p">,</span>
  <span class="na">day</span>          <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">institution</span>  <span class="p">=</span> <span class="s">{Università La Sapienza di Roma -- PSTP Technoscience}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective}}</span><span class="p">,</span>
  <span class="na">note</span>         <span class="p">=</span> <span class="s">{Blogpost}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Francesco Pappone</name></author><category term="transformers"/><category term="attention"/><category term="graph neural networks"/><summary type="html"><![CDATA[Why causal transformers naturally concentrate attention on their earliest tokens.]]></summary></entry><entry><title type="html">Model Merging — a biased overview</title><link href="https://crisostomi.github.io/blog/2025/model_merging/" rel="alternate" type="text/html" title="Model Merging — a biased overview"/><published>2025-08-17T00:00:00+00:00</published><updated>2025-08-17T00:00:00+00:00</updated><id>https://crisostomi.github.io/blog/2025/model_merging</id><content type="html" xml:base="https://crisostomi.github.io/blog/2025/model_merging/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/weight_space-480.webp 480w,/blog/assets/img/blog/weight_space-800.webp 800w,/blog/assets/img/blog/weight_space-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/weight_space.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The HuggingFace Universe. Credits to the <a href="https://weight-space-learning.github.io/">Workshop on Weight Space Learning</a></figcaption> </figure> <aside class="notice"> <strong>Disclaimer</strong>: This is not a survey. It's more of a tour where my own work keeps getting suspiciously good seats. I promise I'll try a more balanced and comprehensive one in the future. </aside> <h2 id="motivation">Motivation</h2> <p>I recently attended an <a href="https://estimathon.com/">Estimathon</a> game. This is basically a quiz where you estimate for some hard-to-quantify question like <em>“How many cabs are there in New York?”</em><d-footnote>Around 12,000 <a href="https://en.wikipedia.org/wiki/Taxis_of_New_York_City">according to Wikipedia.</a></d-footnote> without using any tools. I was a total disaster. But let me ask you one Estimathon-style question I just made up:</p> <blockquote> <p><em>“How many models were there on HuggingFace one year ago?”</em></p> </blockquote> <p>Take a moment to think before scrolling. Got your answer? Nice. Click to reveal the answer.</p> <details><summary>Number of models on HuggingFace last year 👀</summary> <p>Answer: <strong>841,347</strong> models.</p> </details> <p>Were you close? No? Okay, another chance:</p> <blockquote> <p><em>“How many models are there on HuggingFace today?”</em></p> </blockquote> <p>As above, think about it. You’re basically guessing the growth rate of HuggingFace itself.</p> <details><summary>Number of models on HuggingFace today 👀</summary> <p>Answer: <strong>1,957,743</strong> models.</p> </details> <p>Almost got it this time? Cool, you win a t-shirt or something. As you can see, the number has <strong>more than doubled</strong> in just one year! <d-footnote>The number of models refers to the date of publication, i.e. August 18th 2025. Last year refers precisely to August 18th 2024, obtained via the <a href="https://web.archive.org/">WayBack Machine</a>.</d-footnote></p> <p>With this explosion of models, a natural question comes up: should we keep making new ones, or spend more effort reusing what we already have? If, like me, you lean toward the latter in many practical cases, this blogpost is for you. Apparently, that’s also the view at <em>Thinking Machines Lab</em> (which just closed a $2B seed round), where they plan to <em>“combine neural network layers from a range of open-source models with a technique that is similar to model merging”</em>.<d-footnote>See <a href="https://www.theinformation.com/articles/ex-openai-cto-muratis-startup-plans-compete-openai-others">The Information</a>. Sorry, paywalled!</d-footnote></p> <p>But even if you just want to make the GPUs go <em>brrr</em> and focus on training and tuning as much as possible, model merging might still be for you. ByteDance found merging effective for LLM pretraining <d-cite key="Yunshui2025-qz"></d-cite><d-footnote>See also <a href="https://x.com/giffmana/status/1924849877634449878">Lucas Beyer’s excellent summary</a>.</d-footnote>. Cohere’s latest CMD-A report backed that up. <d-cite key="cohere2025commandaenterprisereadylarge"></d-cite>.</p> <p>Enough motivation, let’s merge!</p> <hr/> <h2 id="intro-to-model-merging">Intro to model merging</h2> <p>First things first, what is <em>model merging</em>? Informally, it’s any technique that, given two or more models (the endpoints) and produces a new one that preserves their capabilities. Typically, we want the merging process to be data-free, and the resulting model to have:</p> <ol> <li>The same number of parameters as the endpoints.</li> <li>No extra runtime overhead.</li> </ol> <p>These requirements are not always enforced. Depending on the use case, it might make sense to relax one of them — for example, allowing <a href="#routing-and-moerging">extra overhead for routing</a> or using a small dataset to find <a href="#llms-and-evolutionary-merging">better merging coefficients</a>.</p> <p>Okay, but <em>what</em> models are we merging, and <em>why</em>? It’s helpful to think in terms of two broad categories. The subdivision is not strict, but it makes the landscape easier to navigate:</p> <ul> <li><strong>Merging models trained from scratch on the same task</strong> — covering linear mode connectivity, neuron permutation symmetries, and permutation matching.</li> <li><strong>Merging models finetuned from the same base model on different tasks</strong> — including task vectors, structure-aware merging methods, and evolutionary merging of LLMs.</li> </ul> <hr/> <h2 id="merging-models-trained-from-scratch-on-the-same-task">Merging models trained from scratch on the same task</h2> <p>The setup is simple: we start with two models initialized differently, $\theta^A_{\text{init}}$ and $\theta^B_{\text{init}}$. We train both on the same task $t$, then merge them into a new model $\theta_t^{A,B}$. Since the task index doesn’t add much information in this context, we drop it.</p> <h3 id="mode-connectivity">Mode connectivity</h3> <p>It all began with <strong>mode connectivity</strong>.</p> <p>People once thought that parameters corresponding to different minima (modes) were <strong>isolated</strong> in the loss landscape, each mode living in its own valley (<strong>basin</strong>). This was bad news: if true, you couldn’t move between minima without incurring a sharp loss increase. But can these modes actually be connected in weight space via a low-loss (or high-accuracy) path?</p> <aside> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/loss_landscape_plot-480.webp 480w,/blog/assets/img/blog/loss_landscape_plot-800.webp 800w,/blog/assets/img/blog/loss_landscape_plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/loss_landscape_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> Two modes in their basins. </p> </aside> <p>Turns out the answer is <strong>yes</strong>:</p> <ul> <li>Yes, and this allows us to interpolate between them to obtain cheap ensembles <d-cite key="Garipov2018-pz"></d-cite>.</li> <li>Yes, and in fact a large number of modes lie on a shared low-loss manifold <d-cite key="Draxler2018-vr"></d-cite>.</li> <li>Yes, and if the two modes share an initial training phase, the connecting path may even be linear <d-cite key="linear-mode-connectivity"></d-cite>.</li> </ul> <p>When the connecting path is linear, we usually assume the two modes to live in the same basin. This is kind of a big deal in model merging. Why? Well, because if the two modes can’t be connected linearly (i.e., linear interpolations between the modes result in a high loss), then we can’t average the models. Or, at least, can’t do it this simply. But how can we check if two modes are linearly connected, and to what degree? We will need to compute the loss barrier between the two.</p> \[\underbrace{\max_{\lambda \in [0,1]} \mathcal{L}\!\left((1-\lambda)\theta_A + \lambda \theta_B\right)}_{\text{highest loss along the path}} - \underbrace{\tfrac{1}{2}\Big(\mathcal{L}(\theta_A) + \mathcal{L}(\theta_B)\Big)}_{\text{average loss at endpoints}}\] <p>Here, the first term measures the highest loss encountered along the linear interpolation between the two modes, while the second one is the average loss at the endpoints. If the two modes lie in the same basin, the interpolated loss remains low and the barrier is close to zero. A high loss barrier instead suggests the presence of a peak or saddle point separating the two modes, indicating they belong to different regions of the loss landscape.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/loss_barrier-480.webp 480w,/blog/assets/img/blog/loss_barrier-800.webp 800w,/blog/assets/img/blog/loss_barrier-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/loss_barrier.png" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 400px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now that we know how to measure for closeness in the basin sense, we can move on to one of the key insights in model merging.</p> <hr/> <h3 id="neuron-permutation-symmetries">Neuron permutation symmetries</h3> <p>Consider a generic linear layer $z_{\ell+1} = \sigma(W_{\ell} z_{\ell})$, removing biases for simplicity. Now, what happens if we shuffle the rows of $W_{\ell}$? That’s equivalent to multiplying on the left by a permutation $P$, giving $W_\ell’ = P W_\ell$. The new output is</p> <aside> <p> Biases just need to be permuted along with the corresponding rows. </p> </aside> \[z'_{\ell+1} = \sigma(W_{\ell}' z_{\ell}) = \sigma(P W_{\ell} z_{\ell}) = P \sigma(W_{\ell} z_{\ell}) = P z_{\ell+1}.\] <p>Here, we’ve pulled $P$ outside $\sigma$, possible because nonlinearities act elementwise and commute with permutations.<d-footnote>Intuitively: whether you first shuffle a sequence and then apply an elementwise nonlinearity, or apply the nonlinearity first and then shuffle, the result is the same.</d-footnote></p> <p>So far, the two networks (with $W_{\ell}$ vs $W’_{\ell}$) aren’t the same; their outputs differ by a permutation. But let’s also permute the <strong>columns</strong> of the <em>next</em> layer by $P^\top$. Since permutation matrices are orthogonal ($P^{-1} = P^\top$), the output of the next layer becomes:</p> \[z'_{\ell+2} = \sigma(W_{\ell+1}' z'_{\ell+1}) = \sigma(W_{\ell+1} P^\top P z_{\ell+1}) = \sigma(W_{\ell+1} z_{\ell+1}) = z_{\ell+2}.\] <p>No tricks, the math checks out. The outputs are <strong>identical</strong>.</p> <p>So, are these two networks the same? Internally, no, their weights differ (potentially a lot). But functionally, yes: they compute the exact same mapping.</p> <p>This shows that <strong>neuron permutations create families of functionally equivalent networks.</strong> And since different random seeds lead to different permutations, many distinct-looking weight configurations are actually the <em>same function</em>.</p> <p>This led to the conjecture: once you account for permutations, all modes collapse into a single shared basin <d-cite key="Entezari2021-me"></d-cite>. If this was the case, then we should in principle be able to find, for each layer of model $A$, a permutation of its neurons that maps them to the neurons of the corresponding layer in model $B$, practically teleporting $A$ into $B$’s basin. There, <code class="language-plaintext highlighter-rouge">torch.mean(A, B)</code> is all you need!</p> <hr/> <h3 id="neuron-matching">Neuron matching</h3> <p>We wrapped up the previous section looking for a magical way to align the neurons of two models. One possible objective could be the following</p> \[\arg\max_{\{P_\ell \in \mathbb{P}\}} \sum_{\ell=1}^{L} \left\langle W_\ell^A, \, P_\ell W_\ell^B P_{\ell-1}^{\top} \right\rangle\] <p>where</p> \[\langle A, B \rangle = \mathrm{tr}(A^\top B) = \sum_{i=1}^m \langle A_{i,:}, B_{i,:} \rangle,\] <p>so at layer $\ell$ we are basically looking for the permutation $P_{\ell}$ that best aligns the rows (neurons) of the two matrices in a dot product sense. $P_{\ell}$ is then also applied, after transposition, to the subsequent layer to maintain functional equivalence.</p> <aside> <p>Remember we are searching in the space of functionally equivalent networks.</p> </aside> <p>Like all good things in life, this problem is NP-Hard <d-cite key="git-rebasin"></d-cite>. The Git Re-Basin way, at this point, is to do a layer-wise approximation of the problem, in which the previous and subsequent permutations are held fixed, making each problem a simple Linear Assignment Problem (LAP).<d-footnote>Such a problem allows efficient off-the-shelf algorithms like the <a href="https://en.wikipedia.org/wiki/Hungarian_algorithm">Hungarian algorithm</a>.</d-footnote> Find a (somewhat simplified) version of the algorithm below.</p> <d-code block="" language="python"> <pre><code class="language-python">

def match_neurons(A, B):
    # Initialize permutations as identity
    P = [identity(N) for l in layers]

    for i in num_steps:
        progress = False
        for l in shuffle(layers):

            W_A, W_B = A[l], B[l]

            # Compute neuron-to-neuron similarity matrix under current permutation
            sim = compute_similarity(W_A, P[l] @ W_B @ P[l-1])

            # Solve linear assignment problem
            new_P_l = linear_assignment_problem(sim)

            # Update permutation
            P[l] = new_P_l

            # Recompute similarity
            sim_new = compute_similarity(W_A, P[l] @ W_B @ P[l-1])

            if sim_new.mean() &gt; sim.mean():
                progress = True

        if not progress:
            return P
</code></pre> </d-code> <p>Surprisingly, while Git Re-Basin (2022) is probably the most popular matching algorithm, a precursor (and also, a general case) of this algorithm by S.P. Singh and M. Jaggi was already around in 2019! <d-cite key="singh2020model"></d-cite> There, permutations are replaced by soft maps. These generalize permutations, since a soft map is just a doubly stochastic matrix. For instance a permutation matrix between two sets of $3$ objects would look like</p> \[\textbf{Permutation matrix } P = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\] <p>so each row/column has exactly one 1 (hard assignment), while a soft map may look like</p> \[\textbf{Soft map } S = \begin{bmatrix} 0.7 &amp; 0.3 &amp; 0.0 \\ 0.2 &amp; 0.5 &amp; 0.3 \\ 0.1 &amp; 0.2 &amp; 0.7 \end{bmatrix}\] <p>so rows/columns sum to 1, entries in $[0,1]$ (fractional assignment). Soft maps are the bread and butter of optimal transport.</p> <hr/> <h3 id="entering-cycle-consistency">Entering cycle-consistency</h3> <p>Now entering my own work: I’ll try to keep it fair and high level, so I don’t monopolize the overview. I can do this. The question here is: what happens if we want to match and merge more than 2 models? The easiest solution would be to choose one reference model, find pairwise maps to it, and then aggregate everything in this one. Couple of problems: when optimizing for each pair, the optimization is not aware of the other models. This means that the maps do not compose gracefully: if one maps from a model A to B, then to C and back to A, the end result would be way different than the starting point. In other words, the maps <strong>lack cycle-consistency</strong>. The other (main) problem is that the reference model is an arbitrary choice. And we know, arbitrariness is never good as it opens the way to variance in the results: depending on a (often random) choice might change the results by double digits. Let’s try something else! In $C^2M^3$ <d-cite key="cycle-consistent"></d-cite> we start from the <a href="#neuron-matching">standard weight matching equation</a> and consider all the possible pairs of models</p> \[\arg\max_{\{P_i^{pq} \in \mathbb{P}\}} \sum_{\substack{(p,q)}} \;\; \sum_{i=1}^{L} \left\langle W_i^p,\; P_i^{pq} W_i^q \left(P_{i-1}^{pq}\right)^{\top} \right\rangle\] <p>We factorize each permutation $P^{BA}$ (mapping $A$ to $B$) as the composition of two permutations: one mapping $A$ to a universe space, and one mapping from the universe back to $B$. Since each time you have to pass through the universe, you end-up with a series of permutations that cancel each other out, eventually composing to the identity and guaranteeing cycle-consistency.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/pairwise-vs-cyclecons-480.webp 480w,/blog/assets/img/blog/pairwise-vs-cyclecons-800.webp 800w,/blog/assets/img/blog/pairwise-vs-cyclecons-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/pairwise-vs-cyclecons.png" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The optimization problem we obtain is similar, just a tiny bit uglier due to the factorizations. We optimize this one with Frank-Wolfe<d-footnote>Frank-Wolfe considers a linear approximation of the objective function, and moves towards a minimizer of this linear function. Works for problems with convex solution sets and convex objective functions.</d-footnote>, check the paper for the full details! For brevity, let’s just say merging in the universe space works very well: modes are much more connected there than they originally were!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/basins_after_mapping.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/basins_after_mapping.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="merging-models-finetuned-from-the-same-base-model-on-different-tasks">Merging models finetuned from the same base model on different tasks</h2> <p>We now switch setup. Tabula rasa. This time we start from a common pretrained model $\theta_{\text{pt}}$, which is finetuned separately on different tasks $t_1, t_2$ (say, MNIST and CIFAR-100) to obtain $\theta_{t_1}$ and $\theta_{t_2}$. Our goal is to combine them into a single model $\theta_{t_1,t_2}$ that:</p> <aside>Notation note: I sometimes use $\theta_{\text{pt}}$ or $\theta_{\text{base}}$ to mean the same thing: the pretrained foundation model before finetuning.</aside> <ul> <li>Has the same number of parameters as the base.</li> <li>Can do both tasks at once.</li> </ul> <p>As before, we want this merging process to be data-free. Picture downloading two finetuned checkpoints from HuggingFace and fusing them into a single multi-task model without touching the original datasets.</p> <aside>You often don't even have access to finetuning datasets.</aside> <p>So how do we do it?</p> <h3 id="task-arithmetic">Task arithmetic</h3> <p>The story begins with a simple yet powerful observation: finetuning on a task looks like <em>adding a vector</em> in weight space. This is trivially true when you consider the difference between the finetuned model and its base. Let us denote the update induced by task $t$ as the <strong>task vector</strong></p> \[\tau_t = \theta_t - \theta_{\text{pt}}.\] <p>Then, finetuning is nothing more than</p> \[\theta_t = \theta_{\text{pt}} + \tau_t.\] <p>This way of writing things suggests an almost irresistible idea: if we want a model that can do both $t_1$ and $t_2$, why not just add their updates?</p> \[\theta_{t_1,t_2} = \theta_{\text{pt}} + \tau_{t_1} + \tau_{t_2}.\] <p>And there you have it: task arithmetic <d-cite key="task-vectors"></d-cite>.</p> <p>Of course, life isn’t quite that easy. Sometimes adding updates works beautifully, sometimes it leads to catastrophic interference, and sometimes you get a weird in-between model that can’t do either of the tasks. Still, the arithmetic view was an important first step: it made explicit that task updates behave <em>vectorially</em>, and therefore can be combined, compared, and even algebraically manipulated.</p> <p>So, in general, merging models through task arithmetic boils down to</p> \[\theta_{\text{MT}} = \theta_{\text{pt}} + \alpha \sum_{t \in T} \tau_t\] <p>where $\alpha$ is a scaling factor that is usually optimized on a validation set.</p> <h3 id="task-vectors-and-gradients">Task vectors and gradients</h3> <p>When I first heard about task arithmetic, I was puzzled: why should adding two random finetuning updates give anything meaningful? Let’s do a simple thought experiment.</p> <p>Suppose you finetune a pretrained model with vanilla gradient descent, no minibatches, and only a <em>single epoch</em>. In this case, the task vector is exactly the negative gradient of the loss at the base model:</p> \[\tau_t = - \eta \, \nabla \overline{\mathcal{L}}_t(\theta_{\text{pt}}),\] <p>where $\eta$ is the learning rate and $\overline{\mathcal{L}}_t$ denotes the average loss over task $t$.</p> <p>Now, if we add task vectors from multiple tasks, linearity of the gradient gives</p> \[\sum_{t \in T} \tau_t = \sum_{t \in T} \Big(- \eta \, \nabla \overline{\mathcal{L}}_t(\theta_{\text{pt}})\Big) = - \eta \, \nabla \Bigg(\sum_{t \in T} \overline{\mathcal{L}}_t\Bigg) = - \eta \, \nabla \overline{\mathcal{L}}_T(\theta_{\text{pt}}).\] <p>In other words, adding task vectors is (in this restricted setting) equivalent to taking a single <strong>multi-task gradient step</strong> from the pretrained model!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/task_vector_vs_gradient-480.webp 480w,/blog/assets/img/blog/task_vector_vs_gradient-800.webp 800w,/blog/assets/img/blog/task_vector_vs_gradient-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/blog/assets/img/blog/task_vector_vs_gradient.png" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>What about the scaling factor $\alpha$ we introduced earlier? In this view, $\alpha$ simply plays the role of the learning rate: tuning it adjusts how far you move along the combined gradient.</p> <p>Of course, this equivalence only holds in a very idealized setting (no minibatches, one pass, no curvature). With minibatches or multiple epochs, the connection weakens. Still, this perspective grounds task arithmetic in optimization, moving it away from magic vector algebra and toward something more principled. In <d-cite key="zhou2025taskvectors"></d-cite> we show to what degree this relation holds in practice, check it out!</p> <h3 id="structure-aware-merging-methods">Structure-aware merging methods</h3> <p>So up until now we have considered task vectors as flat vectors, so these $\theta$s live in $\mathbb{R}^n$, and so does $\tau$</p> \[\theta_{\text{MT}} = \theta_{\text{pt}} + \alpha \sum_{t \in T} \tau_t\] <p>But we all know that $\theta$ are not really flat vectors right? And neither should be $\tau$, since at some layers it may have a matrix or tensor structure.</p> <p>So we first consider these differences layer-wise, defining layer-wise task matrices instead of the global task vectors we’ve discussed so far.</p> \[\theta_{\text{MT}}^{(l)} = \theta_{\text{pt}}^{(l)} + \alpha \sum_{t \in T} \Delta_t^{(l)}\] <p>And ask ourselves, can we actually leverage this structure? As it is often the case, we will start by studying the SVD of layers having a matrix structure.</p> \[\Delta = U \Sigma V^\top = \sum_{i=1}^{r} \sigma_i \, u_i v_i^\top\] <p>where $r$ is the rank of the matrix. The next question follows naturally: are $\Delta$s low-rank? i.e., we try to approximate $\Delta$ with</p> \[\tilde{\Delta} = \tilde{U} \tilde{\Sigma} \tilde{V}^\top = \sum_{i=1}^{k} \sigma_i \, u_i v_i^\top \quad k \ll r.\] <p>Of course they are!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/deltas-low-rank.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/deltas-low-rank.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 500px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Ok cool then I guess we can just sum the low-rank approximations $\tilde{\Delta}$ and solve merging… right? Not so fast. Check this out.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/similarity_matrices_sing_vectors.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/similarity_matrices_sing_vectors.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 300px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Basically there is a strong interplay (measured as the dot product) between the singular vectors from different tasks, and this induces interference in the merging. What we do then in Task Singular Vectors <d-cite key="TSV"></d-cite>, is we orthogonalize these across tasks before merging through Procrustes orthogonalization.<d-footnote>Don't use Gram-Schmidt, we wasted months on that!</d-footnote> The results you get this way were fairly hard to believe at first.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/radar-charts.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/radar-charts.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" style=" max-width: 600px; " data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="routing-and-moerging">Routing and MoErging</h3> <p>Wrapping up Task Singular Vectors <d-cite key="TSV"></d-cite>, the final merged update is given by</p> \[\Delta_{\text{MT}} = \sum_{t \in T} \sum_{i=1}^{k} \sigma_i \, u_i v_i^\top\] <p>Since the merging process is a one-time, data-free step, there is not much we can do at this point. But what if we relax one of our original assumptions and instead allow some extra inference-time compute? Intuitively, it would be great if we could select only the singular vectors for the right task</p> \[\Delta_{\mathrm{MT}}=\sum_{i=1}^{T} \underset{[i=h]}{\mathbb{1}} \sum_{j=1}^k \sigma_j^i u_j^i v_j^{i \top}=\sum_{j=1}^k \sigma_j^h u_j^h v_j^{h \top}=\hat{\Delta}_h\] <p>as this would be equivalent to using the low-rank approximated $\tilde{\Delta}$, which we’ve seen preserves something like $99.5\%$ of the accuracy! Even if we could just restrict the set of selected tasks to $K$ instead of $N$ it would be something. Now, I know what you’re thinking.. a router. But didn’t we want to be data-free? turns out we can <d-cite key="mass"></d-cite>.</p> <aside> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/projection.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/projection.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </aside> <p>Say that we embed our sample $x$ with some model (e.g. one we previously merged) to obtain $z_\ell$. What we can do now is to compute the residual obtained from its projection onto each task-specific subspace as spanned by the corresponding singular vectors $V_t$</p> \[r_t \gets \| z_\ell - V_t V_t^\top z_\ell \|_2.\] <p>This gives us a vector of unnormalized logits, which (surprise surprise) we can use to compute a softmax over the tasks to get the distribution we wanted. Optionally, we can threshold the resulting probabilities and choose a maximum $K$ defining the number of tasks that can be selected. We can now only merge these ones and use the resulting model for inference!</p> <p>Why should the right singular vectors of the $\Delta$s be a proper choice for task classification? It works very well in practice, and we show some cool experiments in the paper. I’ll try to write a blogpost about it soon.</p> <h3 id="llms-and-evolutionary-merging">LLMs and Evolutionary Merging</h3> <p>Ok this all sounds cool, but what about LLMs? Well, it turns out that for these ones, task arithmetic already works quite well. There are some slightly more sophisticated methods which I will hopefully write about when I expand the blog, but for the moment let’s just say that they don’t usually result in double-digit improvements.</p> <p>Things change, however, if we drop the data-free constraint. In this case, results can be significantly better. One notable example is a recent paper from Sakana AI, proposing to use evolutionary algorithms to search for the best combination coefficients <d-cite key="akiba2025evolutionary"> </d-cite>. The results are quite impressive: they manage to synthesize a new Japanese LLM with state-of-the-art math-solving skills by merging a Japanese Mistral-7B finetuning with some math-specific finetunings.<d-footnote>Clearly, state-of-the-art in answering math questions in Japanese; merging is cool but it doesn't create new knowledge out of thin air.</d-footnote></p> <p>Evolutionary algorithms are cool, but they are not particularly famous for their efficiency. Just think that the framework involves a loop like the following</p> <d-code block="" language="python"> <pre><code class="language-python">

for step in steps:
  for model in population:
    fitness = eval(model, dataset)

</code></pre> </d-code> <p>where <code class="language-plaintext highlighter-rouge">eval</code> is a function that evaluates each model (LLM with a gazillion parameters) on the dataset and returns a fitness score. Each call might take hours. Multiply those hours by the number of models in each population and the number of steps, and you get a pretty good idea of the total compute cost involved. Merging is a pretty democratic tool, with hundreds of thousands of models being merged and uploaded on HuggingFace by everyday users on consumer GPUs, or even without accelerated hardware. So we ask ourselves, can we preserve some of the end performance of evolutionary merging but still allow common users to use it? The answer, against all expectations by the reader, is yes. Enter MERGE$^3$ <d-cite key="mencattini2025merge"></d-cite>. Intuitively, if we had a way to significantly shrink the evaluation dataset, we could make this process much more efficient. What if we had a way to estimate the accuracy of a model over a whole dataset given just a few of its samples? Say, 20? That’s precisely the goal of <em>Item Response Theory</em> (IRT). To do this, we use the logistic model proposed by tinyBenchmarks <d-cite key="polotinybenchmarks"></d-cite>. With this framework, we can approximate the model’s accuracy as the probability that the model is correct averaged over all the samples in the dataset</p> \[\operatorname{accuracy}(m) \approx \frac{1}{N} \sum_{i=1}^N \mathbb{P}(Y_{im} = 1 \, | \, {\color{OliveGreen}\gamma_m}, {\color{YellowOrange}\alpha_i}, {\color{Cyan}\beta_i}) = \frac{1}{N} \sum_{i=1}^N \frac{1}{1 + \exp(- {\color{YellowOrange}\alpha_i}^\top {\color{OliveGreen}\gamma_m} + {\color{Cyan}\beta_i})}.\] <p>This probability depends upon three sets of parameters, $\alpha$s, $\beta$s and $\gamma$s. For a model $m$, ${\color{OliveGreen}\gamma_m}$ encodes its latent ability, while ${\color{YellowOrange}\alpha_i}$ selects what abilities are required for a particular sample. So the higher the alignment between ${\color{YellowOrange}\alpha}$ and ${\color{OliveGreen}\gamma}$, the higher the probability. The difficulty parameter ${\color{Cyan}\beta_i}$ then acts as a kind of per-sample threshold that shifts the center of the probability, i.e. the value that your alignment must have to reach a probability of 0.5. Without it, this would be just a standard logistic function, with probability 0.5 at logits equal to 0.</p> <p>Now, ${\color{YellowOrange}\alpha}$s and ${\color{Cyan}\beta}$s can be precomputed robustly over the dataset with different models, but ${\color{OliveGreen}\gamma}$ is model-specific. During the evolution we therefore have to estimate ${\color{OliveGreen}\gamma}$ only on the small subsample. Our key insight is that, being the model at hand a combination of the endpoints, also its abilities will be some combination of those of the endpoints. In particular, we assume this combination to be linear, and instead of fitting ${\color{OliveGreen}\gamma}$ directly, we fit its interpolation coefficients</p> \[{\color{OliveGreen}\gamma_{\tilde{m}}} = \sum_{j=1}^m \lambda_j \,{\color{OliveGreen}\gamma_{j}}\] <p>When we plug this new estimator into the evolution pipeline, we obtain results close to those obtained with the full dataset, but at a fraction of the computational cost. Indeed, the experiments that follow were run on an Nvidia 4090 in just a day!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/assets/img/blog/merge3_results.svg" sizes="95vw"/> <img src="/blog/assets/img/blog/merge3_results.svg" class="mx-auto img-fluid rounded d-block" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="what-comes-next">What comes next?</h2> <p>Looking at the big picture, the motivations for merging diverge a bit across domains. In computer vision, the main driver has been compression: taking multiple models trained on the same or related tasks and recycling their parameters into a more compact form. In language, by contrast, merging is mostly about compositionality: fusing different finetunings so the result can do a task that neither of the endpoints could do. Put simply: in vision, it’s fine if the merged model is just the sum of its parts; in language, we often hope for something greater than the sum.</p> <p>A natural, unsurprising goal for compression-oriented model merging is to reach the multi-task upper bound. Multi-task learning still suffers from task interference, but backpropagation at least helps mitigate it.</p> <p>Beyond that, some frontiers are still wide open. Heterogeneous merging (combining models with different architectures, widths, or parameter counts) remains largely unsolved. Most existing approaches assume architectural homogeneity, with the occasional exception of optimal-transport–based methods that are less rigid. Cracking this problem would unlock unprecedented reuse.</p> <p>Finally, we’re still mostly blind when it comes to understanding why merging works. In some cases, merges succeed spectacularly; in others, they fail badly. Could tools from mechanistic interpretability come handy here?</p> <p>I’ll try to continuously improve this blogpost, but for today that’s about as far as I can take you. Until next time, happy merging!</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>Of course, when I say my work, I really mean the collective effort of many brilliant collaborators<d-footnote>Listed in alphabetical order by last name.</d-footnote>: Daniele Baieri, Florian Bernard, Maria Sofia Bucarelli, Giuseppe Alessio D’Inverno, Marco Fumero, Antonio Andrea Gargiulo, Iacopo Masi, Tommaso Mencattini, Robert Adrian Minut, Emanuele Rodolà, Andrea Santilli, Simone Scardapane, Fabrizio Silvestri, Daniele Solombrino, Luca Zhou, Alessandro Zirilli.</p> <p>Thank you!</p>]]></content><author><name>Donato Crisostomi</name></author><category term="model merging"/><category term="machine learning"/><category term="research"/><summary type="html"><![CDATA[A friendly tour of model merging, suspiciously aligned with my own research.]]></summary></entry></feed>